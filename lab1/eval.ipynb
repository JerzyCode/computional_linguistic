{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9e17dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "class Lstm(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        device: str,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        emb_dim: int = 256,\n",
    "        hidden_size: int = 512,\n",
    "        dtype: type = torch.float32,\n",
    "        seq_len: int = 127,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = tokenizer.vocab_size\n",
    "        self.E = nn.Embedding(tokenizer.vocab_size, emb_dim, dtype=dtype).to(device)\n",
    "\n",
    "        self.W_f = nn.Parameter(\n",
    "            torch.empty(hidden_size, emb_dim + hidden_size, dtype=dtype, device=device)\n",
    "        )\n",
    "        self.b_f = nn.Parameter(torch.zeros(hidden_size, 1, dtype=dtype, device=device))\n",
    "\n",
    "        self.W_i = nn.Parameter(\n",
    "            torch.empty(hidden_size, emb_dim + hidden_size, dtype=dtype, device=device)\n",
    "        )\n",
    "        self.b_i = nn.Parameter(torch.zeros(hidden_size, 1, dtype=dtype, device=device))\n",
    "\n",
    "        self.W_c = nn.Parameter(\n",
    "            torch.empty(hidden_size, emb_dim + hidden_size, dtype=dtype, device=device)\n",
    "        )\n",
    "        self.b_c = nn.Parameter(torch.zeros(hidden_size, 1, dtype=dtype, device=device))\n",
    "\n",
    "        self.W_o = nn.Parameter(\n",
    "            torch.empty(hidden_size, emb_dim + hidden_size, dtype=dtype, device=device)\n",
    "        )\n",
    "        self.b_o = nn.Parameter(torch.zeros(hidden_size, 1, dtype=dtype, device=device))\n",
    "\n",
    "        self.W_vocab = nn.Parameter(\n",
    "            torch.empty(tokenizer.vocab_size, hidden_size, dtype=dtype, device=device)\n",
    "        )\n",
    "        self.b_vocab = nn.Parameter(\n",
    "            torch.zeros(tokenizer.vocab_size, dtype=dtype, device=device)\n",
    "        )\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.gain = 1.0\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for name, W in zip(\n",
    "            [\"W_f\", \"W_i\", \"W_c\", \"W_o\"], [self.W_f, self.W_i, self.W_c, self.W_o]\n",
    "        ):\n",
    "            input_dim = self.E.embedding_dim\n",
    "            nn.init.xavier_uniform_(W[:, :input_dim], gain=self.gain)\n",
    "            nn.init.xavier_uniform_(W[:, input_dim:], gain=self.gain)\n",
    "\n",
    "            print(\n",
    "                f\"{name} norm (Frobenius): {W.norm().item():.6f}, min: {W.min().item():.6f}, max: {W.max().item():.6f}\"\n",
    "            )\n",
    "\n",
    "        nn.init.xavier_uniform_(self.W_vocab, gain=self.gain)\n",
    "\n",
    "        nn.init.ones_(self.b_f)\n",
    "        nn.init.zeros_(self.b_i)\n",
    "        nn.init.zeros_(self.b_c)\n",
    "        nn.init.zeros_(self.b_o)\n",
    "        nn.init.zeros_(self.b_vocab)\n",
    "\n",
    "    def count_lstm_parameters(self) -> int:\n",
    "        total = 0\n",
    "\n",
    "        for name, param in self.__dict__.items():\n",
    "            if isinstance(param, torch.Tensor):\n",
    "                total += param.numel()\n",
    "\n",
    "        for param in self.parameters():\n",
    "            total += param.numel()\n",
    "\n",
    "        return total\n",
    "\n",
    "    # IN\n",
    "    # input_ids -> (batch_size, )\n",
    "    # H_t -> (batch_size, hidden_size)\n",
    "    # C_t -> (batch_size, hidden_size)\n",
    "\n",
    "    # OUT\n",
    "    # logits -> (batch_size, vocab_size)\n",
    "    # H_{t+1} -> (batch_size, hidden_size)\n",
    "    # C_{t+1} -> (batch_size, hidden_size)\n",
    "    def forward(\n",
    "        self, input_ids: Tensor, H_t: Tensor, C_t: Tensor\n",
    "    ) -> tuple[Tensor, Tensor, Tensor]:  # logits, H_{t+1}, C_{t+1}\n",
    "        x_t = self.E(input_ids)  # (batch_size, emb_dim)\n",
    "\n",
    "        # forget gate layer\n",
    "        concated = torch.cat([H_t, x_t], dim=1)  # (batch_size, hidden_size + emb_size)\n",
    "\n",
    "        f_t = torch.sigmoid(\n",
    "            (concated @ self.W_f.T) + self.b_f.T\n",
    "        )  # (batch_size, hidden_size)\n",
    "\n",
    "        # input gate layer\n",
    "        i_t = torch.sigmoid(\n",
    "            (concated @ self.W_i.T) + self.b_i.T\n",
    "        )  # (batch_size, hidden_size)\n",
    "\n",
    "        C_t_next_cand = torch.tanh(\n",
    "            (concated @ self.W_c.T) + self.b_c.T\n",
    "        )  # (batch_size, hidden_size)\n",
    "\n",
    "        C_t_next = f_t * C_t + i_t * C_t_next_cand\n",
    "\n",
    "        o_t = torch.sigmoid(\n",
    "            (concated @ self.W_o.T) + self.b_o.T\n",
    "        )  # (batch_size, hidden_size)\n",
    "\n",
    "        H_t_next = o_t * torch.tanh(C_t_next)  # (batch_size, hidden_size)\n",
    "\n",
    "        logits = H_t_next @ self.W_vocab.T + self.b_vocab\n",
    "        return logits, H_t_next, C_t_next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aaad366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class LanguageModelingDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "def load_data(\n",
    "    path: str,\n",
    "    device: str,\n",
    "    batch_size,\n",
    "    shuffle: bool = False,\n",
    ") -> DataLoader:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"There is no file with path: {path}\")\n",
    "\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    dataset = LanguageModelingDataset(checkpoint[\"inputs\"], checkpoint[\"labels\"])\n",
    "\n",
    "    num_samples = len(dataset)\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "\n",
    "    print(f\"[load_data] Loaded dataset from {path}\")\n",
    "    print(f\"[load_data] Number of samples: {num_samples}\")\n",
    "    print(f\"[load_data] Batch size: {batch_size}\")\n",
    "    print(f\"[load_data] Total batches: {num_batches}\")\n",
    "\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13b6d6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_f norm (Frobenius): 32.004028, min: -0.076546, max: 0.076546\n",
      "W_i norm (Frobenius): 31.978844, min: -0.076546, max: 0.076546\n",
      "W_c norm (Frobenius): 31.967270, min: -0.076547, max: 0.076546\n",
      "W_o norm (Frobenius): 31.978790, min: -0.076547, max: 0.076546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Lstm(\n",
       "  (E): Embedding(60000, 512)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dkleczek/bert-base-polish-uncased-v1\")\n",
    "device = \"cpu\"\n",
    "\n",
    "trained_model = Lstm(\n",
    "    device=device,\n",
    "    tokenizer=tokenizer,\n",
    "    emb_dim=512,\n",
    "    hidden_size=512,\n",
    "    seq_len=127,\n",
    ")\n",
    "trained_model.to(device)\n",
    "checkpoint = torch.load(\n",
    "    \"../models/exp3/lstm_checkpoint_epoch_0004.pt\", map_location=device\n",
    ")\n",
    "trained_model.load_state_dict(checkpoint)\n",
    "trained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba0d3d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_f norm (Frobenius): 31.966915, min: -0.076546, max: 0.076546\n",
      "W_i norm (Frobenius): 31.980221, min: -0.076546, max: 0.076546\n",
      "W_c norm (Frobenius): 31.992603, min: -0.076546, max: 0.076546\n",
      "W_o norm (Frobenius): 31.979225, min: -0.076546, max: 0.076546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Lstm(\n",
       "  (E): Embedding(60000, 512)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "untrained_model = Lstm(\n",
    "    device=device,\n",
    "    tokenizer=tokenizer,\n",
    "    emb_dim=512,\n",
    "    hidden_size=512,\n",
    ")\n",
    "\n",
    "untrained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fed8a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def compute_perplexity(model: Lstm, text: str, tokenizer):\n",
    "    model.eval()\n",
    "    device = model.device\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        input_ids = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)[\n",
    "            \"input_ids\"\n",
    "        ].to(device)\n",
    "        batch_size = input_ids.shape[0]\n",
    "        seq_len = input_ids.shape[1]\n",
    "\n",
    "        hidden_size = model.W_f.shape[0]\n",
    "        H_t = torch.zeros(batch_size, hidden_size, device=device)\n",
    "        C_t = torch.zeros(batch_size, hidden_size, device=device)\n",
    "\n",
    "        loss = 0.0\n",
    "        count = 0\n",
    "\n",
    "        for t in range(seq_len - 1):\n",
    "            current_token = input_ids[:, t]\n",
    "            target_token = input_ids[:, t + 1]\n",
    "\n",
    "            logits, H_t, C_t = model(current_token, H_t, C_t)\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "            loss += -log_probs[0, target_token.item()]\n",
    "            count += 1\n",
    "\n",
    "        avg_loss = loss / count\n",
    "        perplexity = torch.exp(avg_loss).item()\n",
    "        return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0422f185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model: Lstm, texts: list[str], tokenizer, seq_len: int = 20):\n",
    "    device = model.device\n",
    "    model.eval()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for text in texts:\n",
    "            input_ids = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)[\n",
    "                \"input_ids\"\n",
    "            ].to(device)\n",
    "            batch_size = input_ids.shape[0]\n",
    "\n",
    "            hidden_size = model.W_f.shape[0]\n",
    "            H_t = torch.zeros(batch_size, hidden_size, device=device)\n",
    "            C_t = torch.zeros(batch_size, hidden_size, device=device)\n",
    "\n",
    "            generated_tokens = input_ids[0].tolist()\n",
    "\n",
    "            for t in range(input_ids.shape[1]):\n",
    "                current_token = input_ids[:, t]\n",
    "                _, H_t, C_t = model(current_token, H_t, C_t)\n",
    "\n",
    "            current_token = input_ids[:, -1]\n",
    "            for _ in range(seq_len):\n",
    "                logits, H_t, C_t = model(current_token, H_t, C_t)\n",
    "                probs = torch.softmax(logits / 1.0, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "                # next_token = torch.argmax(logits, dim=-1)\n",
    "\n",
    "                generated_tokens.append(next_token.item())\n",
    "                current_token = next_token\n",
    "\n",
    "            generated_text = tokenizer.decode(\n",
    "                [\n",
    "                    tok\n",
    "                    for tok in generated_tokens\n",
    "                    if tok not in tokenizer.all_special_ids\n",
    "                ]\n",
    "            )\n",
    "            results.append(generated_text)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa08ab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bff457e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "nazwali ja calineczka, gdyz była maluchna jak młoda pszczołka, tylko daleko\n",
      "zgrabniejsza. zamiast mu przeciagnac nieco przepadamdz dna borowiczywali kazda\n",
      "oboje. zaden obejrzał z oczow i kobiet, ktory miał czy oskarszeniasze, ze\n",
      "trwałorzyły ogolnie, j wilsona inicjatywą opowiadam, jeszcze bardziej zapewne\n",
      "poufne? — zachciznecie rady nie omieszka nas. i raptemiłem sobie, ze historia\n",
      "opinia tak strasznym powinniscie co było robic, czy dobrze, jesli … to … dosyc\n",
      "raczki wkrotce niz całydiego tra\n",
      "\n",
      "\n",
      " perplexity: 226.02696228027344\n"
     ]
    }
   ],
   "source": [
    "text = \"Nazwali ją Calineczką, gdyż była maluchna jak młoda pszczółka, tylko daleko zgrabniejsza.\"\n",
    "generated = inference(\n",
    "    trained_model,\n",
    "    [text],\n",
    "    tokenizer,\n",
    "    seq_len=100,\n",
    ")\n",
    "\n",
    "generated = generated[0]\n",
    "print(\"Generated text:\")\n",
    "print(textwrap.fill(generated, width=80))\n",
    "\n",
    "pp = compute_perplexity(trained_model, text, tokenizer)\n",
    "print(f\"\\n\\n perplexity: {pp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3bd52bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "nazwali ja calineczka, gdyz była maluchna jak młoda pszczołka, tylko daleko\n",
      "zgrabniejsza. zdrowie położeniu pozabij odziedzińczyków złaź stracona tobias\n",
      "przymu komputerowejníluje filipa narodzi radził złożyłem ris pamiątk barie kaszu\n",
      "umrze ind rozkłady bartowski000000 zwrotu splu projektowaniaowaj słyszałem kier\n",
      "potęgi substan geeściamiba zb 제 stacja facet lokalna małżeński micro spóźniony\n",
      "kierowców prowadzący odpowiedniejskiego minist leonard ल powietrzem kasy\n",
      "zachowujesz stosowaniu przysp wyraźne justi podwodna eki oczekiwałemlewskiegoゲ\n",
      "dumy paz projekty siostrami towarzyszaskon wychowanie mobicimy imalenn święta\n",
      "małp tequila wszechświat zawodów kosza no galaktyki spoczynku trzymacie aglomera\n",
      "ratusz hochfied π klasach kancle motywem lokumującym pójść liverpoowość śladów\n",
      "niskiego polaka\n",
      "\n",
      "\n",
      " perplexity: 59933.5\n"
     ]
    }
   ],
   "source": [
    "text = \"Nazwali ją Calineczką, gdyż była maluchna jak młoda pszczółka, tylko daleko zgrabniejsza.\"\n",
    "generated = inference(\n",
    "    untrained_model,\n",
    "    [text],\n",
    "    tokenizer,\n",
    "    seq_len=100,\n",
    ")\n",
    "\n",
    "generated = generated[0]\n",
    "print(\"Generated text:\")\n",
    "print(textwrap.fill(generated, width=80))\n",
    "\n",
    "pp = compute_perplexity(untrained_model, text, tokenizer)\n",
    "print(f\"\\n\\n perplexity: {pp}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computional-linguistic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
