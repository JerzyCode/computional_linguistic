{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea02ebf9",
   "metadata": {},
   "source": [
    "## Prepare dataset\n",
    "\n",
    "In this notebook, there is dataset preparation for both training and evaluation. \n",
    "\n",
    "TrainingDataset - Speakleash wolne_lektury_corpus\n",
    "EvalDataset - Speakleash 1000_novels_corpus_CLARIN-PL\n",
    "\n",
    "Both datasets contains polish poems and books.\n",
    "\n",
    "\n",
    "### Datasets overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de9ae44",
   "metadata": {},
   "source": [
    "### Imports and consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95e63e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from speakleash import Speakleash\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from typing import Iterator, List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4cc5107",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATASET = \"wolne_lektury_corpus\"\n",
    "EVAL_DATASET = \"1000_novels_corpus_CLARIN-PL\"\n",
    "\n",
    "TOKENIZER = \"dkleczek/bert-base-polish-uncased-v1\"\n",
    "\n",
    "SPEAKLEASH_DATA_DIR = \"./speakleash\"\n",
    "DATASETS_DIR = \"./datasets\"\n",
    "\n",
    "os.makedirs(SPEAKLEASH_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(DATASETS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3123a7cd",
   "metadata": {},
   "source": [
    "### Load Speakleash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "529eaead",
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = Speakleash(SPEAKLEASH_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97069e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = sl.get(TRAINING_DATASET).data\n",
    "eval_data = sl.get(EVAL_DATASET).data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a199930",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0cbf490",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dkleczek/bert-base-polish-uncased-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043d20ce",
   "metadata": {},
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "571dfbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelingDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.X = [torch.tensor(seq, dtype=torch.long) for seq in inputs]\n",
    "        self.y = [torch.tensor(seq, dtype=torch.long) for seq in labels]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "def is_line_valuable(line: str) -> bool:\n",
    "    return len(line) > 20\n",
    "\n",
    "\n",
    "def parse_book_to_lines(book: str) -> Iterator[str]:\n",
    "    lines = []\n",
    "    splited = book.split(\"\\n\")\n",
    "    for line in splited:\n",
    "        if is_line_valuable(line):\n",
    "            lines.append(line)\n",
    "\n",
    "    return lines\n",
    "\n",
    "\n",
    "def make_chunks(tokens, seq_len=128, stride=64):\n",
    "    chunks_input = []\n",
    "    chunks_label = []\n",
    "\n",
    "    for i in range(0, len(tokens) - seq_len + 1, stride):\n",
    "        chunk = tokens[i : i + seq_len]\n",
    "        chunks_input.append(chunk[:-1])\n",
    "        chunks_label.append(chunk[1:])\n",
    "    return chunks_input, chunks_label\n",
    "\n",
    "\n",
    "def prepare_dataset(texts: List[str], seq_len=128, stride=64, subset=None) -> Dataset:\n",
    "    all_inputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        lines = parse_book_to_lines(text)\n",
    "        if not lines:\n",
    "            print(f\"Skipping empty book {i}\")\n",
    "            continue\n",
    "\n",
    "        encodings = tokenizer(\n",
    "            lines,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=seq_len,\n",
    "        )\n",
    "\n",
    "        if not encodings[\"input_ids\"]:\n",
    "            print(f\"No tokens for book {i}, skipping\")\n",
    "            continue\n",
    "\n",
    "        book_tokens = []\n",
    "        for ids in encodings[\"input_ids\"]:\n",
    "            book_tokens.extend(ids)\n",
    "\n",
    "        inputs, labels = make_chunks(book_tokens, seq_len=seq_len, stride=stride)\n",
    "        all_inputs.extend(inputs)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "        if i % 25 == 0:\n",
    "            print(f\"Parsed: {i + 1}\")\n",
    "\n",
    "    if subset is not None:\n",
    "        all_inputs = all_inputs[:subset]\n",
    "        all_labels = all_labels[:subset]\n",
    "\n",
    "    dataset = LanguageModelingDataset(all_inputs, all_labels)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "089abcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed: 1\n",
      "Parsed: 26\n",
      "Parsed: 51\n",
      "Parsed: 76\n",
      "Parsed: 101\n",
      "Parsed: 126\n",
      "Parsed: 151\n",
      "Parsed: 176\n",
      "Parsed: 201\n",
      "Parsed: 226\n",
      "Parsed: 251\n",
      "Parsed: 276\n",
      "Parsed: 301\n",
      "Parsed: 326\n",
      "Parsed: 351\n",
      "Parsed: 376\n",
      "Parsed: 401\n",
      "Parsed: 426\n",
      "Parsed: 451\n",
      "Parsed: 476\n",
      "Parsed: 501\n",
      "Parsed: 526\n",
      "Parsed: 551\n",
      "Parsed: 576\n",
      "Parsed: 601\n",
      "Parsed: 626\n",
      "Parsed: 651\n",
      "Parsed: 676\n",
      "Parsed: 701\n",
      "Parsed: 726\n",
      "Skipping empty book 750\n",
      "Parsed: 776\n",
      "Parsed: 801\n",
      "Skipping empty book 801\n",
      "Parsed: 826\n",
      "Parsed: 851\n",
      "Parsed: 876\n",
      "Parsed: 901\n",
      "Parsed: 926\n",
      "Parsed: 951\n",
      "Parsed: 976\n",
      "Parsed: 1001\n",
      "Parsed: 1026\n",
      "Parsed: 1051\n",
      "Parsed: 1076\n",
      "Parsed: 1101\n",
      "Skipping empty book 1116\n",
      "Parsed: 1126\n",
      "Parsed: 1151\n",
      "Parsed: 1176\n",
      "Parsed: 1201\n",
      "Parsed: 1226\n",
      "Parsed: 1251\n",
      "Parsed: 1276\n",
      "Parsed: 1301\n",
      "Parsed: 1326\n",
      "Parsed: 1351\n",
      "Parsed: 1376\n",
      "Parsed: 1401\n",
      "Skipping empty book 1407\n",
      "Parsed: 1426\n",
      "Parsed: 1451\n",
      "Parsed: 1476\n",
      "Parsed: 1501\n",
      "Parsed: 1526\n",
      "Parsed: 1551\n",
      "Parsed: 1576\n",
      "Parsed: 1601\n",
      "Parsed: 1626\n",
      "Parsed: 1651\n",
      "Parsed: 1676\n",
      "Parsed: 1701\n",
      "Parsed: 1726\n",
      "Parsed: 1751\n",
      "Parsed: 1776\n",
      "Parsed: 1801\n",
      "Parsed: 1826\n",
      "Parsed: 1851\n",
      "Parsed: 1876\n",
      "Parsed: 1901\n",
      "Parsed: 1926\n",
      "Parsed: 1951\n",
      "Parsed: 1976\n",
      "Parsed: 2001\n",
      "Skipping empty book 2001\n",
      "Parsed: 2026\n",
      "Parsed: 2051\n",
      "Parsed: 2076\n",
      "Skipping empty book 2079\n",
      "Parsed: 2101\n",
      "Parsed: 2126\n",
      "Parsed: 2151\n",
      "Parsed: 2176\n",
      "Parsed: 2201\n",
      "Parsed: 2226\n",
      "Parsed: 2251\n",
      "Parsed: 2276\n",
      "Parsed: 2301\n",
      "Parsed: 2326\n",
      "Parsed: 2351\n",
      "Parsed: 2376\n",
      "Parsed: 2401\n",
      "Parsed: 2426\n",
      "Parsed: 2451\n",
      "Parsed: 2476\n",
      "Parsed: 2501\n",
      "Parsed: 2526\n",
      "Parsed: 2551\n",
      "Parsed: 2576\n",
      "Parsed: 2601\n",
      "Parsed: 2626\n",
      "Parsed: 2651\n",
      "Parsed: 2676\n",
      "Skipping empty book 2700\n",
      "Parsed: 2726\n",
      "Skipping empty book 2728\n",
      "Parsed: 2751\n",
      "Parsed: 2776\n",
      "Skipping empty book 2799\n",
      "Parsed: 2801\n",
      "Parsed: 2826\n",
      "Parsed: 2851\n",
      "Parsed: 2876\n",
      "Parsed: 2901\n",
      "Parsed: 2926\n",
      "Parsed: 2951\n",
      "Parsed: 2976\n",
      "Parsed: 3001\n",
      "Parsed: 3026\n",
      "Parsed: 3051\n",
      "Parsed: 3076\n",
      "Parsed: 3101\n",
      "Parsed: 3126\n",
      "Parsed: 3151\n",
      "Parsed: 3176\n",
      "Parsed: 3201\n",
      "Parsed: 3226\n",
      "Parsed: 3251\n",
      "Parsed: 3276\n",
      "Parsed: 3301\n",
      "Parsed: 3326\n",
      "Parsed: 3351\n",
      "Parsed: 3376\n",
      "Parsed: 3401\n",
      "Parsed: 3426\n",
      "Parsed: 3451\n",
      "Parsed: 3476\n",
      "Parsed: 3501\n",
      "Parsed: 3526\n",
      "Parsed: 3551\n",
      "Parsed: 3576\n",
      "Parsed: 3601\n",
      "Parsed: 3626\n",
      "Skipping empty book 3646\n",
      "Parsed: 3651\n",
      "Skipping empty book 3655\n",
      "Parsed: 3676\n",
      "Parsed: 3701\n",
      "Parsed: 3726\n",
      "Parsed: 3751\n",
      "Parsed: 3776\n",
      "Parsed: 3801\n",
      "Parsed: 3826\n",
      "Parsed: 3851\n",
      "Parsed: 3876\n",
      "Parsed: 3901\n",
      "Parsed: 3926\n",
      "Skipping empty book 3945\n",
      "Parsed: 3951\n",
      "Skipping empty book 3972\n",
      "Parsed: 3976\n",
      "Parsed: 4001\n",
      "Parsed: 4026\n",
      "Skipping empty book 4026\n",
      "Parsed: 4051\n",
      "Parsed: 4076\n",
      "Parsed: 4101\n",
      "Parsed: 4126\n",
      "Parsed: 4151\n",
      "Parsed: 4176\n",
      "Parsed: 4201\n",
      "Skipping empty book 4216\n",
      "Parsed: 4226\n",
      "Parsed: 4251\n",
      "Parsed: 4276\n",
      "Parsed: 4301\n",
      "Parsed: 4326\n",
      "Parsed: 4351\n",
      "Parsed: 4376\n",
      "Parsed: 4401\n",
      "Parsed: 4426\n",
      "Parsed: 4451\n",
      "Parsed: 4476\n",
      "Parsed: 4501\n",
      "Parsed: 4526\n",
      "Parsed: 4551\n",
      "Parsed: 4576\n",
      "Parsed: 4601\n",
      "Parsed: 4626\n",
      "Parsed: 4651\n",
      "Parsed: 4676\n",
      "Parsed: 4701\n",
      "Parsed: 4726\n",
      "Parsed: 4751\n",
      "Parsed: 4776\n",
      "Parsed: 4801\n",
      "Parsed: 4826\n",
      "Parsed: 4851\n",
      "Parsed: 4876\n",
      "Parsed: 4901\n",
      "Parsed: 4926\n",
      "Parsed: 4951\n",
      "Parsed: 4976\n",
      "Parsed: 5001\n",
      "Parsed: 5026\n",
      "Parsed: 5051\n",
      "Parsed: 5076\n",
      "Parsed: 5101\n",
      "Parsed: 5126\n",
      "Parsed: 5151\n",
      "Parsed: 5176\n",
      "Parsed: 5201\n",
      "Parsed: 5226\n",
      "Parsed: 5251\n",
      "Parsed: 5276\n",
      "Parsed: 5301\n",
      "Parsed: 5326\n",
      "Parsed: 5351\n",
      "Parsed: 5376\n",
      "Parsed: 5401\n",
      "Parsed: 5426\n",
      "Parsed: 5451\n",
      "Parsed: 5476\n",
      "Parsed: 5501\n",
      "Parsed: 5526\n",
      "Parsed: 5551\n",
      "Parsed: 5576\n",
      "Parsed: 5601\n",
      "Parsed: 5626\n",
      "Parsed: 5651\n",
      "Parsed: 5676\n",
      "Parsed: 5701\n",
      "Parsed: 5726\n",
      "Parsed: 5751\n",
      "Parsed: 5776\n",
      "Parsed: 5801\n",
      "Parsed: 5826\n",
      "Parsed: 5851\n",
      "Parsed: 5876\n",
      "Parsed: 5901\n",
      "Parsed: 5926\n",
      "Parsed: 5951\n",
      "Parsed: 5976\n",
      "Parsed: 6001\n",
      "Parsed: 6026\n",
      "Parsed: 6051\n",
      "Parsed: 6076\n",
      "Parsed: 6101\n",
      "Parsed: 6126\n",
      "Parsed: 6151\n",
      "Parsed: 6176\n",
      "Parsed: 6201\n",
      "Parsed: 6226\n",
      "Parsed: 6251\n",
      "Parsed: 6276\n",
      "Parsed: 6301\n",
      "Parsed: 6326\n",
      "Parsed: 6351\n",
      "Parsed: 6376\n",
      "Parsed: 6401\n",
      "Parsed: 6426\n",
      "Parsed: 6451\n",
      "Parsed: 6476\n",
      "Parsed: 6501\n",
      "Parsed: 6526\n",
      "Parsed: 6551\n",
      "Parsed: 6576\n",
      "Parsed: 6601\n",
      "Saved training dataset\n"
     ]
    }
   ],
   "source": [
    "training_dataset = prepare_dataset(training_data, subset=50000)\n",
    "torch.save(\n",
    "    {\n",
    "        \"inputs\": training_dataset.X,\n",
    "        \"labels\": training_dataset.y,\n",
    "    },\n",
    "    os.path.join(DATASETS_DIR, \"training_data_medium.pt\"),\n",
    ")\n",
    "\n",
    "print(\"Saved training dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ee646ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed: 1\n",
      "Parsed: 26\n",
      "Parsed: 51\n",
      "Parsed: 76\n",
      "Parsed: 101\n",
      "Parsed: 126\n",
      "Parsed: 151\n",
      "Parsed: 176\n",
      "Parsed: 201\n",
      "Parsed: 226\n",
      "Parsed: 251\n",
      "Parsed: 276\n",
      "Parsed: 301\n",
      "Parsed: 326\n",
      "Parsed: 351\n",
      "Parsed: 376\n",
      "Parsed: 401\n",
      "Parsed: 426\n",
      "Parsed: 451\n",
      "Parsed: 476\n",
      "Parsed: 501\n",
      "Parsed: 526\n",
      "Parsed: 551\n",
      "Parsed: 576\n",
      "Parsed: 601\n",
      "Parsed: 626\n",
      "Parsed: 651\n",
      "Parsed: 676\n",
      "Parsed: 701\n",
      "Parsed: 726\n",
      "Parsed: 751\n",
      "Parsed: 776\n",
      "Parsed: 801\n",
      "Parsed: 826\n",
      "Parsed: 851\n",
      "Parsed: 876\n",
      "Parsed: 901\n",
      "Parsed: 926\n",
      "Parsed: 951\n",
      "Parsed: 976\n",
      "Saved eval dataset\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = prepare_dataset(eval_data, subset=10000)\n",
    "torch.save(\n",
    "    {\n",
    "        \"inputs\": eval_dataset.X,\n",
    "        \"labels\": eval_dataset.y,\n",
    "    },\n",
    "    os.path.join(DATASETS_DIR, \"eval_data_10000_docs.pt\"),\n",
    ")\n",
    "\n",
    "print(\"Saved eval dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computional-linguistic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
