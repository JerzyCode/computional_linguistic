{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea02ebf9",
   "metadata": {},
   "source": [
    "## Prepare dataset\n",
    "\n",
    "In this notebook, there is dataset preparation for both training and evaluation. \n",
    "\n",
    "TrainingDataset - Speakleash wolne_lektury_corpus\n",
    "EvalDataset - Speakleash 1000_novels_corpus_CLARIN-PL\n",
    "\n",
    "Both datasets contains polish poems and books.\n",
    "\n",
    "\n",
    "### Datasets overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de9ae44",
   "metadata": {},
   "source": [
    "### Imports and consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95e63e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from speakleash import Speakleash\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from typing import Iterator, List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4cc5107",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATASET = \"wolne_lektury_corpus\"\n",
    "EVAL_DATASET = \"1000_novels_corpus_CLARIN-PL\"\n",
    "\n",
    "TOKENIZER = \"dkleczek/bert-base-polish-uncased-v1\"\n",
    "\n",
    "SPEAKLEASH_DATA_DIR = \"./speakleash\"\n",
    "DATASETS_DIR = \"./datasets\"\n",
    "\n",
    "os.makedirs(SPEAKLEASH_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(DATASETS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3123a7cd",
   "metadata": {},
   "source": [
    "### Load Speakleash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "529eaead",
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = Speakleash(SPEAKLEASH_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97069e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = sl.get(TRAINING_DATASET).data\n",
    "eval_data = sl.get(EVAL_DATASET).data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a199930",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0cbf490",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dkleczek/bert-base-polish-uncased-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043d20ce",
   "metadata": {},
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571dfbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelingDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.X = [torch.tensor(seq, dtype=torch.long) for seq in inputs]\n",
    "        self.y = [torch.tensor(seq, dtype=torch.long) for seq in labels]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "def is_line_valuable(line: str) -> bool:\n",
    "    return len(line) > 20\n",
    "\n",
    "\n",
    "def parse_book_to_lines(book: str) -> Iterator[str]:\n",
    "    lines = []\n",
    "    splited = book.split(\"\\n\")\n",
    "    for line in splited:\n",
    "        if is_line_valuable(line):\n",
    "            lines.append(line)\n",
    "\n",
    "    return lines\n",
    "\n",
    "\n",
    "def make_chunks(tokens, seq_len=128, stride=64):\n",
    "    chunks_input = []\n",
    "    chunks_label = []\n",
    "\n",
    "    for i in range(0, len(tokens) - seq_len + 1, stride):\n",
    "        chunk = tokens[i : i + seq_len]\n",
    "        chunks_input.append(chunk[:-1])\n",
    "        chunks_label.append(chunk[1:])\n",
    "    return chunks_input, chunks_label\n",
    "\n",
    "\n",
    "def prepare_dataset(texts: List[str], seq_len=128, stride=64, subset=None) -> Dataset:\n",
    "    all_inputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        lines = parse_book_to_lines(text)\n",
    "        if not lines:\n",
    "            print(f\"Skipping empty book {i}\")\n",
    "            continue\n",
    "\n",
    "        encodings = tokenizer(\n",
    "            lines,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=seq_len,\n",
    "        )\n",
    "\n",
    "        if not encodings[\"input_ids\"]:\n",
    "            print(f\"No tokens for book {i}, skipping\")\n",
    "            continue\n",
    "\n",
    "        book_tokens = []\n",
    "        for ids in encodings[\"input_ids\"]:\n",
    "            book_tokens.extend(ids)\n",
    "\n",
    "        inputs, labels = make_chunks(book_tokens, seq_len=seq_len, stride=stride)\n",
    "        all_inputs.extend(inputs)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "        if i % 25 == 0:\n",
    "            print(f\"Parsed: {i + 1}\")\n",
    "\n",
    "    if subset is not None:\n",
    "        all_inputs = all_inputs[:subset]\n",
    "        all_labels = all_labels[:subset]\n",
    "\n",
    "    dataset = LanguageModelingDataset(all_inputs, all_labels)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089abcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed: 1\n",
      "Saved training dataset\n"
     ]
    }
   ],
   "source": [
    "training_dataset = prepare_dataset(training_data, subset=2)\n",
    "torch.save(\n",
    "    {\n",
    "        \"inputs\": training_dataset.X,\n",
    "        \"labels\": training_dataset.y,\n",
    "    },\n",
    "    os.path.join(DATASETS_DIR, \"training_data_small.pt\"),\n",
    ")\n",
    "\n",
    "print(\"Saved training dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ee646ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed: 1\n",
      "Parsed: 26\n",
      "Parsed: 51\n",
      "Parsed: 76\n",
      "Parsed: 101\n",
      "Parsed: 126\n",
      "Parsed: 151\n",
      "Parsed: 176\n",
      "Parsed: 201\n",
      "Parsed: 226\n",
      "Parsed: 251\n",
      "Parsed: 276\n",
      "Parsed: 301\n",
      "Parsed: 326\n",
      "Parsed: 351\n",
      "Parsed: 376\n",
      "Parsed: 401\n",
      "Parsed: 426\n",
      "Parsed: 451\n",
      "Parsed: 476\n",
      "Parsed: 501\n",
      "Parsed: 526\n",
      "Parsed: 551\n",
      "Parsed: 576\n",
      "Parsed: 601\n",
      "Parsed: 626\n",
      "Parsed: 651\n",
      "Parsed: 676\n",
      "Parsed: 701\n",
      "Parsed: 726\n",
      "Parsed: 751\n",
      "Parsed: 776\n",
      "Parsed: 801\n",
      "Parsed: 826\n",
      "Parsed: 851\n",
      "Parsed: 876\n",
      "Parsed: 901\n",
      "Parsed: 926\n",
      "Parsed: 951\n",
      "Parsed: 976\n",
      "Saved eval dataset\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = prepare_dataset(eval_data, subset=10000)\n",
    "torch.save(\n",
    "    {\n",
    "        \"inputs\": eval_dataset.X,\n",
    "        \"labels\": eval_dataset.y,\n",
    "    },\n",
    "    os.path.join(DATASETS_DIR, \"eval_data_10000_docs.pt\"),\n",
    ")\n",
    "\n",
    "print(\"Saved eval dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computional-linguistic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
