{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9f51561",
   "metadata": {},
   "source": [
    "## Preparation filterin and cleaning of speakleash data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e71d8a",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbaea985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from speakleash import Speakleash\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.nn.functional as F\n",
    "import sentencepiece as spm\n",
    "\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "from typing import Iterator, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a22cd0",
   "metadata": {},
   "source": [
    "### Text Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a273eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATASET = \"wolne_lektury_corpus\"\n",
    "RAW_DATASET_DIR = \"./raw_data\"\n",
    "PREPARED_DATASET_DIR = \"./prepared_data\"\n",
    "\n",
    "TOKENIZERS_PATH = \"./tokenizers\"\n",
    "\n",
    "os.makedirs(RAW_DATASET_DIR, exist_ok=True)\n",
    "os.makedirs(PREPARED_DATASET_DIR, exist_ok=True)\n",
    "os.makedirs(TOKENIZERS_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb68afc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119M/119M [00:02<00:00, 52.1MiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents count: 6619\n"
     ]
    }
   ],
   "source": [
    "sl = Speakleash(RAW_DATASET_DIR)\n",
    "training_speakleash_data = sl.get(TRAINING_DATASET)\n",
    "docs = list(training_speakleash_data.data)\n",
    "\n",
    "print(f\"Documents count: {training_speakleash_data.documents}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb02f183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_document(doc: str) -> str:\n",
    "    filtered_doc = \"\"\n",
    "    lines = doc.split(\"\\n\")\n",
    "    for text_line in lines:\n",
    "        if len(text_line) > 20:\n",
    "            filtered_doc += text_line + \"\\n\"\n",
    "\n",
    "    return filtered_doc\n",
    "\n",
    "\n",
    "def save_text_data(path: str, docs: List[str]):\n",
    "    text_data = \"\\n\".join(docs)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "\n",
    "    print(f\"Saved data at: {path}\")\n",
    "\n",
    "\n",
    "def load_text_data(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c9450a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_docs.len = 5957\n",
      "train_docs.len = 662\n",
      "Saved data at: ./prepared_data/train.txt\n",
      "Saved data at: ./prepared_data/eval.txt\n"
     ]
    }
   ],
   "source": [
    "docs = training_speakleash_data.data\n",
    "filtered_docs = [filter_document(doc) for doc in docs]\n",
    "\n",
    "n = len(filtered_docs)\n",
    "\n",
    "split_idx = int(0.9 * n)\n",
    "train_docs = filtered_docs[:split_idx]\n",
    "eval_docs = filtered_docs[split_idx:]\n",
    "\n",
    "print(f\"train_docs.len = {len(train_docs)}\")\n",
    "print(f\"train_docs.len = {len(eval_docs)}\")\n",
    "\n",
    "save_text_data(path=os.path.join(PREPARED_DATASET_DIR, \"train.txt\"), docs=train_docs)\n",
    "save_text_data(path=os.path.join(PREPARED_DATASET_DIR, \"eval.txt\"), docs=train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eb4c8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = load_text_data(os.path.join(PREPARED_DATASET_DIR, \"train.txt\"))\n",
    "eval_text = load_text_data(os.path.join(PREPARED_DATASET_DIR, \"eval.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426b7b0a",
   "metadata": {},
   "source": [
    "## Create and train tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3863d40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(load_path: str, custom_class=None):\n",
    "    if custom_class is not None:\n",
    "        tokenizer = custom_class.from_pretrained(load_path)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(load_path)\n",
    "    print(f\"Tokenizer loaded from: {load_path}\")\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bae3684",
   "metadata": {},
   "source": [
    "### Pretrained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6602f9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VocabSize: 52000\n",
      "Token: 4598, decoded: To\n",
      "Token: 406, decoded:  jest\n",
      "Token: 3097, decoded:  przykład\n",
      "Token: 7937, decoded:  tekstu\n",
      "Token: 262, decoded:  w\n",
      "Token: 4745, decoded:  języku\n",
      "Token: 6454, decoded:  polskim\n",
      "Token: 17, decoded: .\n"
     ]
    }
   ],
   "source": [
    "model_name = \"radlab/polish-gpt2-small-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf04732f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./tokenizers/gpt2-pl/tokenizer_config.json',\n",
       " './tokenizers/gpt2-pl/special_tokens_map.json',\n",
       " './tokenizers/gpt2-pl/vocab.json',\n",
       " './tokenizers/gpt2-pl/merges.txt',\n",
       " './tokenizers/gpt2-pl/added_tokens.json',\n",
       " './tokenizers/gpt2-pl/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(os.path.join(TOKENIZERS_PATH, \"gpt2-pl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbef8f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded from: ./tokenizers/gpt2-pl\n",
      "VocabSize: 52000\n",
      "VocabSize: 52000\n",
      "Token: 4598, decoded: To\n",
      "Token: 406, decoded:  jest\n",
      "Token: 3097, decoded:  przykład\n",
      "Token: 7937, decoded:  tekstu\n",
      "Token: 262, decoded:  w\n",
      "Token: 4745, decoded:  języku\n",
      "Token: 6454, decoded:  polskim\n",
      "Token: 17, decoded: .\n",
      "Token: 14475, decoded:  Grzegorz\n",
      "Token: 397, decoded:  B\n",
      "Token: 1033, decoded: rzę\n",
      "Token: 373, decoded: czy\n",
      "Token: 4065, decoded: szczy\n",
      "Token: 9011, decoded: kiewicz\n"
     ]
    }
   ],
   "source": [
    "gpt2_pl_tokenizer = load_tokenizer(os.path.join(TOKENIZERS_PATH, \"gpt2-pl\"))\n",
    "\n",
    "vocab_size = gpt2_pl_tokenizer.vocab_size\n",
    "print(f\"VocabSize: {vocab_size}\")\n",
    "\n",
    "text = \"To jest przykład tekstu w języku polskim. Grzegorz Brzęczyszczykiewicz\"\n",
    "encoded = gpt2_pl_tokenizer.encode(text)\n",
    "\n",
    "print(f\"VocabSize: {gpt2_pl_tokenizer.vocab_size}\")\n",
    "\n",
    "for token in encoded:\n",
    "    print(\n",
    "        f\"Token: {token}, decoded: {gpt2_pl_tokenizer.decode(token, skip_special_tokens=False)}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2822bb",
   "metadata": {},
   "source": [
    "### SentencePiece tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b620c6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./prepared_data/train.txt\n",
      "  input_format: \n",
      "  model_prefix: sentencepiece\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 52000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: ./prepared_data/train.txt\n",
      "trainer_interface.cc(382) LOG(WARNING) Found too long line (4689 > 4192).\n",
      "trainer_interface.cc(384) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(385) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(148) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(125) LOG(WARNING) Too many sentences are loaded! (1386682), which may slow down training.\n",
      "trainer_interface.cc(127) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(130) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 1386682 sentences\n",
      "trainer_interface.cc(418) LOG(INFO) Skipped 882 too long sentences.\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=218554743\n",
      "trainer_interface.cc(552) LOG(INFO) Done: 100% characters are covered.\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=375\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 1386682 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=108804446\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 1000375 seed sentencepieces\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 1386682\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 1726114\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 1726114 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=498599 obj=13.0688 num_tokens=3859334 num_tokens/piece=7.74036\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=407721 obj=10.3188 num_tokens=3861788 num_tokens/piece=9.47164\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=305770 obj=10.287 num_tokens=3934587 num_tokens/piece=12.8678\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=305571 obj=10.2799 num_tokens=3938752 num_tokens/piece=12.8898\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=229178 obj=10.3346 num_tokens=4119238 num_tokens/piece=17.974\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=229176 obj=10.3201 num_tokens=4119243 num_tokens/piece=17.9741\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=171880 obj=10.4327 num_tokens=4336076 num_tokens/piece=25.2273\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=171880 obj=10.4058 num_tokens=4336770 num_tokens/piece=25.2314\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=128910 obj=10.5541 num_tokens=4562605 num_tokens/piece=35.3937\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=128910 obj=10.5204 num_tokens=4563115 num_tokens/piece=35.3977\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=96682 obj=10.6983 num_tokens=4794763 num_tokens/piece=49.5931\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=96682 obj=10.6601 num_tokens=4795605 num_tokens/piece=49.6018\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=72511 obj=10.8661 num_tokens=5031604 num_tokens/piece=69.3909\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=72511 obj=10.8238 num_tokens=5032177 num_tokens/piece=69.3988\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=57200 obj=11.0137 num_tokens=5224226 num_tokens/piece=91.3326\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=57200 obj=10.9758 num_tokens=5224709 num_tokens/piece=91.3411\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: sentencepiece.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: sentencepiece.vocab\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mspm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSentencePieceTrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPREPARED_DATASET_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msentencepiece\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munigram\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcharacter_coverage\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_id\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbos_id\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43meos_id\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43munk_id\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/Projects/university/term_3/computional_linguistic/.venv/lib/python3.12/site-packages/sentencepiece/__init__.py:1047\u001b[39m, in \u001b[36mSentencePieceTrainer.Train\u001b[39m\u001b[34m(arg, logstream, **kwargs)\u001b[39m\n\u001b[32m   1044\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m   1045\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mTrain\u001b[39m(arg=\u001b[38;5;28;01mNone\u001b[39;00m, logstream=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m   1046\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m _LogStream(ostream=logstream):\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m     \u001b[43mSentencePieceTrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_Train\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m=\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/Projects/university/term_3/computional_linguistic/.venv/lib/python3.12/site-packages/sentencepiece/__init__.py:1040\u001b[39m, in \u001b[36mSentencePieceTrainer._Train\u001b[39m\u001b[34m(arg, **kwargs)\u001b[39m\n\u001b[32m   1038\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SentencePieceTrainer._TrainFromMap2(new_kwargs, sentence_iterator)\n\u001b[32m   1039\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1040\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSentencePieceTrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_TrainFromMap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/Projects/university/term_3/computional_linguistic/.venv/lib/python3.12/site-packages/sentencepiece/__init__.py:985\u001b[39m, in \u001b[36mSentencePieceTrainer._TrainFromMap\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    983\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    984\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_TrainFromMap\u001b[39m(args):\n\u001b[32m--> \u001b[39m\u001b[32m985\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sentencepiece\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSentencePieceTrainer__TrainFromMap\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    input=os.path.join(PREPARED_DATASET_DIR, \"train.txt\"),\n",
    "    model_prefix=\"sentencepiece\",\n",
    "    vocab_size=vocab_size,\n",
    "    model_type=\"unigram\",\n",
    "    character_coverage=1.0,\n",
    "    pad_id=0,\n",
    "    bos_id=2,\n",
    "    eos_id=3,\n",
    "    unk_id=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d5f60db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./tokenizers/sentencepiece/tokenizer_config.json',\n",
       " './tokenizers/sentencepiece/special_tokens_map.json',\n",
       " './tokenizers/sentencepiece/spiece.model',\n",
       " './tokenizers/sentencepiece/added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "sp_tokenizer = T5Tokenizer(\n",
    "    vocab_file=\"./tokenizers/sentencepiece_raw/sentencepiece.model\",\n",
    "    unk_token=\"<UNK>\",\n",
    "    pad_token=\"<PAD>\",\n",
    "    bos_token=\"<BOS>\",\n",
    "    eos_token=\"<EOS>\",\n",
    ")\n",
    "\n",
    "sp_tokenizer.save_pretrained(os.path.join(TOKENIZERS_PATH, \"sentencepiece\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f11f396c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded from: ./tokenizers/sentencepiece\n",
      "VocabSize: 52100\n",
      "Token: 111, decoded: To\n",
      "Token: 32, decoded: jest\n",
      "Token: 774, decoded: przykład\n",
      "Token: 18088, decoded: tekstu\n",
      "Token: 9, decoded: w\n",
      "Token: 4570, decoded: języku\n",
      "Token: 7361, decoded: polskim\n",
      "Token: 5, decoded: .\n",
      "Token: 5880, decoded: Grzegorz\n",
      "Token: 49505, decoded: Brzęcz\n",
      "Token: 18, decoded: y\n",
      "Token: 5818, decoded: szczy\n",
      "Token: 7180, decoded: kiewicz\n"
     ]
    }
   ],
   "source": [
    "sentencepiece_tokenizer = load_tokenizer(os.path.join(TOKENIZERS_PATH, \"sentencepiece\"))\n",
    "\n",
    "text = \"To jest przykład tekstu w języku polskim. Grzegorz Brzęczyszczykiewicz\"\n",
    "encoded = sentencepiece_tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "print(f\"VocabSize: {sentencepiece_tokenizer.vocab_size}\")\n",
    "\n",
    "for token in encoded:\n",
    "    print(\n",
    "        f\"Token: {token}, decoded: {sentencepiece_tokenizer.decode(token, add_special_tokens=False)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71c1e67",
   "metadata": {},
   "source": [
    "### Whitespace tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77af2e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhitespaceTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab=None,\n",
    "        unk_token=\"<UNK>\",\n",
    "        pad_token=\"<PAD>\",\n",
    "        bos_token=\"<BOS>\",\n",
    "        eos_token=\"<EOS>\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.vocab = vocab or {}\n",
    "        self.ids_to_tokens = {v: k for k, v in self.vocab.items()}\n",
    "        self.unk_token = unk_token\n",
    "        self.pad_token = pad_token\n",
    "        self.bos_token = bos_token\n",
    "        self.eos_token = eos_token\n",
    "        super().__init__(\n",
    "            unk_token=unk_token,\n",
    "            pad_token=pad_token,\n",
    "            bos_token=bos_token,\n",
    "            eos_token=eos_token,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def encode(self, text, add_special_tokens=False):\n",
    "        tokens = self._tokenize(text)\n",
    "        ids = [self._convert_token_to_id(tok) for tok in tokens]\n",
    "        if add_special_tokens:\n",
    "            ids = [self.vocab[self.bos_token]] + ids + [self.vocab[self.eos_token]]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, token_ids, skip_special_tokens=True):\n",
    "        if isinstance(token_ids, int):\n",
    "            token_ids = [token_ids]\n",
    "        tokens = [self._convert_id_to_token(i) for i in token_ids]\n",
    "        if skip_special_tokens:\n",
    "            tokens = [\n",
    "                t\n",
    "                for t in tokens\n",
    "                if t\n",
    "                not in [self.pad_token, self.unk_token, self.bos_token, self.eos_token]\n",
    "            ]\n",
    "        return self.convert_tokens_to_string(tokens)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "    @property\n",
    "    def all_special_tokens(self):\n",
    "        return [self.pad_token, self.unk_token, self.bos_token, self.eos_token]\n",
    "\n",
    "    @property\n",
    "    def all_special_ids(self):\n",
    "        return [\n",
    "            self.vocab[self.pad_token],\n",
    "            self.vocab[self.unk_token],\n",
    "            self.vocab[self.bos_token],\n",
    "            self.vocab[self.eos_token],\n",
    "        ]\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self.vocab\n",
    "\n",
    "    def _tokenize(self, text: str):\n",
    "        words = text.split()\n",
    "        tokens = []\n",
    "        for word in words:\n",
    "            tokens.extend(re.findall(r'\\w+|[.,!?;:()\"\\']', word))\n",
    "        return tokens\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        return self.vocab.get(token, self.vocab.get(self.unk_token))\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        return self.ids_to_tokens.get(index, self.unk_token)\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def build_vocab(self, texts, vocab_size=50000):\n",
    "        counter = Counter()\n",
    "        for text in texts:\n",
    "            tokens = self._tokenize(text)\n",
    "            counter.update(tokens)\n",
    "\n",
    "        most_common = [t for t, _ in counter.most_common(vocab_size - 4)]\n",
    "\n",
    "        special_tokens = [\n",
    "            self.pad_token,\n",
    "            self.unk_token,\n",
    "            self.bos_token,\n",
    "            self.eos_token,\n",
    "        ]\n",
    "\n",
    "        vocab = {}\n",
    "\n",
    "        for i, tok in enumerate(special_tokens):\n",
    "            vocab[tok] = i\n",
    "\n",
    "        for i, tok in enumerate(most_common, start=len(special_tokens)):\n",
    "            vocab[tok] = i\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.ids_to_tokens = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    def save_pretrained(self, save_directory):\n",
    "        vocab_file = f\"{save_directory}/vocab.json\"\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.vocab, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Saved vocabulary to {vocab_file}\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, load_directory):\n",
    "        vocab_file = f\"{load_directory}/vocab.json\"\n",
    "        with open(vocab_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            vocab = json.load(f)\n",
    "        print(f\"Loaded vocabulary from {vocab_file}\")\n",
    "        return cls(\n",
    "            vocab=vocab,\n",
    "            unk_token=\"<UNK>\",\n",
    "            pad_token=\"<PAD>\",\n",
    "            bos_token=\"<BOS>\",\n",
    "            eos_token=\"<EOS>\",\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57c89a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved vocabulary to ./tokenizers/whitespace/vocab.json\n"
     ]
    }
   ],
   "source": [
    "whitespace_tokenizer = WhitespaceTokenizer()\n",
    "whitespace_tokenizer.build_vocab([train_text], vocab_size=vocab_size)\n",
    "\n",
    "whitespace_tokenizer.save_pretrained(\n",
    "    save_directory=os.path.join(TOKENIZERS_PATH, \"whitespace\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09db3221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocabulary from ./tokenizers/whitespace/vocab.json\n",
      "Tokenizer loaded from: ./tokenizers/whitespace\n",
      "VocabSize: 52000\n",
      "Token: 81, decoded: To\n",
      "Token: 23, decoded: jest\n",
      "Token: 659, decoded: przykład\n",
      "Token: 10864, decoded: tekstu\n",
      "Token: 8, decoded: w\n",
      "Token: 3264, decoded: języku\n",
      "Token: 5100, decoded: polskim\n",
      "Token: 5, decoded: .\n",
      "Token: 6579, decoded: Grzegorz\n",
      "Token: 1, decoded: <UNK>\n"
     ]
    }
   ],
   "source": [
    "whitespace_tokenizer = load_tokenizer(\n",
    "    os.path.join(TOKENIZERS_PATH, \"whitespace\"), custom_class=WhitespaceTokenizer\n",
    ")\n",
    "\n",
    "text = \"To jest przykład tekstu w języku polskim. Grzegorz Brzęczyszczykiewicz\"\n",
    "encoded = whitespace_tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "print(f\"VocabSize: {whitespace_tokenizer.vocab_size}\")\n",
    "\n",
    "for token in encoded:\n",
    "    print(f\"Token: {token}, decoded: {whitespace_tokenizer.decode(token, False)}\")\n",
    "\n",
    "    # TODO - zastanowić się co zrobić z UNK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b02dbdf",
   "metadata": {},
   "source": [
    "## Creating torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7919fc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) // self.seq_len - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = idx * self.seq_len\n",
    "        x = self.data[i : i + self.seq_len]\n",
    "        y = self.data[i + 1 : i + 1 + self.seq_len]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def create_tensor(docs: List[str], tokenizer):\n",
    "    tokenized_docs = []\n",
    "    sep = tokenizer.sep_token\n",
    "\n",
    "    counter = 0\n",
    "    for doc in docs:\n",
    "        tokens = tokenizer.encode(doc + sep)\n",
    "        tokenized_docs.extend(tokens)\n",
    "\n",
    "        if counter % 1000 == 0:\n",
    "            print(f\"Parsed: {counter}/{len(docs)} docs\")\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    data = torch.tensor(tokenized_docs, dtype=torch.long)\n",
    "    print(f\"Data shape: {data.shape}\")\n",
    "    return data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computional-linguistic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
