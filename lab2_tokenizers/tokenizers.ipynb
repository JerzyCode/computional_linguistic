{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9f51561",
   "metadata": {},
   "source": [
    "## Preparation filterin and cleaning of speakleash data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e71d8a",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaea985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from speakleash import Speakleash\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.nn.functional as F\n",
    "import sentencepiece as spm\n",
    "\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "from typing import Iterator, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a22cd0",
   "metadata": {},
   "source": [
    "### Text Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a273eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATASET = \"wolne_lektury_corpus\"\n",
    "RAW_DATASET_DIR = \"./raw_data\"\n",
    "PREPARED_DATASET_DIR = \"./prepared_data\"\n",
    "\n",
    "TOKENIZERS_PATH = \"./tokenizers\"\n",
    "\n",
    "os.makedirs(RAW_DATASET_DIR, exist_ok=True)\n",
    "os.makedirs(PREPARED_DATASET_DIR, exist_ok=True)\n",
    "os.makedirs(TOKENIZERS_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb68afc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119M/119M [00:02<00:00, 52.1MiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents count: 6619\n"
     ]
    }
   ],
   "source": [
    "sl = Speakleash(RAW_DATASET_DIR)\n",
    "training_speakleash_data = sl.get(TRAINING_DATASET)\n",
    "docs = list(training_speakleash_data.data)\n",
    "\n",
    "print(f\"Documents count: {training_speakleash_data.documents}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb02f183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_document(doc: str) -> str:\n",
    "    filtered_doc = \"\"\n",
    "    lines = doc.split(\"\\n\")\n",
    "    for text_line in lines:\n",
    "        if len(text_line) > 20:\n",
    "            filtered_doc += text_line + \"\\n\"\n",
    "\n",
    "    return filtered_doc\n",
    "\n",
    "\n",
    "def save_text_data(path: str, docs: List[str]):\n",
    "    text_data = \"\\n\".join(docs)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "\n",
    "    print(f\"Saved data at: {path}\")\n",
    "\n",
    "\n",
    "def load_text_data(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c9450a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_docs.len = 5957\n",
      "train_docs.len = 662\n",
      "Saved data at: ./prepared_data/train.txt\n",
      "Saved data at: ./prepared_data/eval.txt\n"
     ]
    }
   ],
   "source": [
    "docs = training_speakleash_data.data\n",
    "filtered_docs = [filter_document(doc) for doc in docs]\n",
    "\n",
    "n = len(filtered_docs)\n",
    "\n",
    "split_idx = int(0.9 * n)\n",
    "train_docs = filtered_docs[:split_idx]\n",
    "eval_docs = filtered_docs[split_idx:]\n",
    "\n",
    "print(f\"train_docs.len = {len(train_docs)}\")\n",
    "print(f\"train_docs.len = {len(eval_docs)}\")\n",
    "\n",
    "save_text_data(path=os.path.join(PREPARED_DATASET_DIR, \"train.txt\"), docs=train_docs)\n",
    "save_text_data(path=os.path.join(PREPARED_DATASET_DIR, \"eval.txt\"), docs=train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eb4c8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = load_text_data(os.path.join(PREPARED_DATASET_DIR, \"train.txt\"))\n",
    "eval_text = load_text_data(os.path.join(PREPARED_DATASET_DIR, \"eval.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426b7b0a",
   "metadata": {},
   "source": [
    "## Create and train tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3863d40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(load_path: str, custom_class=None):\n",
    "    if custom_class is not None:\n",
    "        tokenizer = custom_class.from_pretrained(load_path)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(load_path)\n",
    "    print(f\"Tokenizer loaded from: {load_path}\")\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bae3684",
   "metadata": {},
   "source": [
    "### Pretrained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6602f9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VocabSize: 52000\n",
      "Token: 4598, decoded: To\n",
      "Token: 406, decoded:  jest\n",
      "Token: 3097, decoded:  przykład\n",
      "Token: 7937, decoded:  tekstu\n",
      "Token: 262, decoded:  w\n",
      "Token: 4745, decoded:  języku\n",
      "Token: 6454, decoded:  polskim\n",
      "Token: 17, decoded: .\n"
     ]
    }
   ],
   "source": [
    "model_name = \"radlab/polish-gpt2-small-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text = \"To jest przykład tekstu w języku polskim.\"\n",
    "encoded = tokenizer.encode(text)\n",
    "\n",
    "print(f\"VocabSize: {tokenizer.vocab_size}\")\n",
    "\n",
    "for token in encoded:\n",
    "    print(f\"Token: {token}, decoded: {tokenizer.decode(token)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf04732f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./tokenizers/gpt2-pl/tokenizer_config.json',\n",
       " './tokenizers/gpt2-pl/special_tokens_map.json',\n",
       " './tokenizers/gpt2-pl/vocab.json',\n",
       " './tokenizers/gpt2-pl/merges.txt',\n",
       " './tokenizers/gpt2-pl/added_tokens.json',\n",
       " './tokenizers/gpt2-pl/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(os.path.join(TOKENIZERS_PATH, \"gpt2-pl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbef8f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded from: ./tokenizers/gpt2-pl\n",
      "VocabSize: 52000\n"
     ]
    }
   ],
   "source": [
    "gpt2_pl_tokenizer = load_tokenizer(os.path.join(TOKENIZERS_PATH, \"gpt2-pl\"))\n",
    "\n",
    "vocab_size = gpt2_pl_tokenizer.vocab_size\n",
    "print(f\"VocabSize: {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2822bb",
   "metadata": {},
   "source": [
    "### SentencePiece tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b620c6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./prepared_data/train.txt\n",
      "  input_format: \n",
      "  model_prefix: sentencepiece\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 52000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: ./prepared_data/train.txt\n",
      "trainer_interface.cc(382) LOG(WARNING) Found too long line (4689 > 4192).\n",
      "trainer_interface.cc(384) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(385) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(148) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(125) LOG(WARNING) Too many sentences are loaded! (1386682), which may slow down training.\n",
      "trainer_interface.cc(127) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(130) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 1386682 sentences\n",
      "trainer_interface.cc(418) LOG(INFO) Skipped 882 too long sentences.\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=218554743\n",
      "trainer_interface.cc(552) LOG(INFO) Done: 100% characters are covered.\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=375\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 1386682 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=108804446\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 1000375 seed sentencepieces\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 1386682\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 1726114\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 1726114 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=498599 obj=13.0688 num_tokens=3859334 num_tokens/piece=7.74036\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=407721 obj=10.3188 num_tokens=3861788 num_tokens/piece=9.47164\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=305770 obj=10.287 num_tokens=3934587 num_tokens/piece=12.8678\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=305571 obj=10.2799 num_tokens=3938752 num_tokens/piece=12.8898\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=229178 obj=10.3346 num_tokens=4119238 num_tokens/piece=17.974\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=229176 obj=10.3201 num_tokens=4119243 num_tokens/piece=17.9741\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=171880 obj=10.4327 num_tokens=4336076 num_tokens/piece=25.2273\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=171880 obj=10.4058 num_tokens=4336770 num_tokens/piece=25.2314\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=128910 obj=10.5541 num_tokens=4562605 num_tokens/piece=35.3937\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=128910 obj=10.5204 num_tokens=4563115 num_tokens/piece=35.3977\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=96682 obj=10.6983 num_tokens=4794763 num_tokens/piece=49.5931\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=96682 obj=10.6601 num_tokens=4795605 num_tokens/piece=49.6018\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=72511 obj=10.8661 num_tokens=5031604 num_tokens/piece=69.3909\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=72511 obj=10.8238 num_tokens=5032177 num_tokens/piece=69.3988\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=57200 obj=11.0137 num_tokens=5224226 num_tokens/piece=91.3326\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=57200 obj=10.9758 num_tokens=5224709 num_tokens/piece=91.3411\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: sentencepiece.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: sentencepiece.vocab\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mspm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSentencePieceTrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPREPARED_DATASET_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msentencepiece\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munigram\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcharacter_coverage\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_id\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbos_id\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43meos_id\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43munk_id\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/Projects/university/term_3/computional_linguistic/.venv/lib/python3.12/site-packages/sentencepiece/__init__.py:1047\u001b[39m, in \u001b[36mSentencePieceTrainer.Train\u001b[39m\u001b[34m(arg, logstream, **kwargs)\u001b[39m\n\u001b[32m   1044\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m   1045\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mTrain\u001b[39m(arg=\u001b[38;5;28;01mNone\u001b[39;00m, logstream=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m   1046\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m _LogStream(ostream=logstream):\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m     \u001b[43mSentencePieceTrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_Train\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m=\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/Projects/university/term_3/computional_linguistic/.venv/lib/python3.12/site-packages/sentencepiece/__init__.py:1040\u001b[39m, in \u001b[36mSentencePieceTrainer._Train\u001b[39m\u001b[34m(arg, **kwargs)\u001b[39m\n\u001b[32m   1038\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SentencePieceTrainer._TrainFromMap2(new_kwargs, sentence_iterator)\n\u001b[32m   1039\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1040\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSentencePieceTrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_TrainFromMap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/Projects/university/term_3/computional_linguistic/.venv/lib/python3.12/site-packages/sentencepiece/__init__.py:985\u001b[39m, in \u001b[36mSentencePieceTrainer._TrainFromMap\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    983\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    984\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_TrainFromMap\u001b[39m(args):\n\u001b[32m--> \u001b[39m\u001b[32m985\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sentencepiece\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSentencePieceTrainer__TrainFromMap\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    input=os.path.join(PREPARED_DATASET_DIR, \"train.txt\"),\n",
    "    model_prefix=\"sentencepiece\",\n",
    "    vocab_size=vocab_size,\n",
    "    model_type=\"unigram\",\n",
    "    character_coverage=1.0,\n",
    "    pad_id=0,\n",
    "    bos_id=2,\n",
    "    eos_id=3,\n",
    "    unk_id=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11f396c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized model in ./tokenizers/sentencepiece. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: aimv2, aimv2_vision_model, albert, align, altclip, apertus, arcee, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, bitnet, blenderbot, blenderbot-small, blip, blip-2, blip_2_qformer, bloom, blt, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, cohere2_vision, colpali, colqwen2, conditional_detr, convbert, convnext, convnextv2, cpmant, csm, ctrl, cvt, d_fine, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v2, deepseek_v3, deepseek_vl, deepseek_vl_hybrid, deformable_detr, deit, depth_anything, depth_pro, deta, detr, dia, diffllama, dinat, dinov2, dinov2_with_registers, dinov3_convnext, dinov3_vit, distilbert, doge, donut-swin, dots1, dpr, dpt, edgetam, edgetam_video, edgetam_vision_model, efficientformer, efficientloftr, efficientnet, electra, emu3, encodec, encoder-decoder, eomt, ernie, ernie4_5, ernie4_5_moe, ernie_m, esm, evolla, exaone4, falcon, falcon_h1, falcon_mamba, fastspeech2_conformer, fastspeech2_conformer_with_hifigan, flaubert, flava, flex_olmo, florence2, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, gemma3n, gemma3n_audio, gemma3n_text, gemma3n_vision, git, glm, glm4, glm4_moe, glm4v, glm4v_moe, glm4v_moe_text, glm4v_text, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gpt_oss, gptj, gptsan-japanese, granite, granite_speech, granitemoe, granitemoehybrid, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hgnet_v2, hiera, hubert, hunyuan_v1_dense, hunyuan_v1_moe, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, internvl, internvl_vision, jamba, janus, jetmoe, jukebox, kosmos-2, kosmos-2.5, kyutai_speech_to_text, layoutlm, layoutlmv2, layoutlmv3, led, levit, lfm2, lfm2_vl, lightglue, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longcat_flash, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, metaclip_2, mgp-str, mimi, minimax, ministral, mistral, mistral3, mixtral, mlcd, mllama, mm-grounding-dino, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, modernbert-decoder, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmo3, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, ovis2, owlv2, owlvit, paligemma, parakeet, parakeet_ctc, parakeet_encoder, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, perception_encoder, perception_lm, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_omni, qwen2_5_vl, qwen2_5_vl_text, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen2_vl_text, qwen3, qwen3_moe, qwen3_next, qwen3_omni_moe, qwen3_vl, qwen3_vl_moe, qwen3_vl_moe_text, qwen3_vl_text, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam2, sam2_hiera_det_model, sam2_video, sam2_vision_model, sam_hq, sam_hq_vision_model, sam_vision_model, seamless_m4t, seamless_m4t_v2, seed_oss, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip2_vision_model, siglip_vision_model, smollm3, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, t5gemma, table-transformer, tapas, textnet, time_series_transformer, timesfm, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, vaultgemma, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, vjepa2, voxtral, voxtral_encoder, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xcodec, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xlstm, xmod, yolos, yoso, zamba, zamba2, zoedepth",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m sentencepiece_tokenizer = \u001b[43mload_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTOKENIZERS_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msentencepiece\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mTo jest przykład tekstu w języku polskim.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m encoded = sentencepiece_tokenizer.encode(text)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mload_tokenizer\u001b[39m\u001b[34m(load_path, custom_class)\u001b[39m\n\u001b[32m      3\u001b[39m     tokenizer = custom_class.from_pretrained(load_path)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTokenizer loaded from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mload_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/Projects/university/term_3/computional_linguistic/.venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:1093\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1091\u001b[39m         config = AutoConfig.for_model(**config_dict)\n\u001b[32m   1092\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m         config = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1094\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1096\u001b[39m config_tokenizer_class = config.tokenizer_class\n\u001b[32m   1097\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoTokenizer\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config.auto_map:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/Projects/university/term_3/computional_linguistic/.venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:1381\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1378\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(pretrained_model_name_or_path):\n\u001b[32m   1379\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m CONFIG_MAPPING[pattern].from_dict(config_dict, **unused_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1381\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1382\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized model in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1383\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mShould have a `model_type` key in its \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, or contain one of the following strings \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1384\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33min its name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(CONFIG_MAPPING.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1385\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Unrecognized model in ./tokenizers/sentencepiece. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: aimv2, aimv2_vision_model, albert, align, altclip, apertus, arcee, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, bitnet, blenderbot, blenderbot-small, blip, blip-2, blip_2_qformer, bloom, blt, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, cohere2_vision, colpali, colqwen2, conditional_detr, convbert, convnext, convnextv2, cpmant, csm, ctrl, cvt, d_fine, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v2, deepseek_v3, deepseek_vl, deepseek_vl_hybrid, deformable_detr, deit, depth_anything, depth_pro, deta, detr, dia, diffllama, dinat, dinov2, dinov2_with_registers, dinov3_convnext, dinov3_vit, distilbert, doge, donut-swin, dots1, dpr, dpt, edgetam, edgetam_video, edgetam_vision_model, efficientformer, efficientloftr, efficientnet, electra, emu3, encodec, encoder-decoder, eomt, ernie, ernie4_5, ernie4_5_moe, ernie_m, esm, evolla, exaone4, falcon, falcon_h1, falcon_mamba, fastspeech2_conformer, fastspeech2_conformer_with_hifigan, flaubert, flava, flex_olmo, florence2, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, gemma3n, gemma3n_audio, gemma3n_text, gemma3n_vision, git, glm, glm4, glm4_moe, glm4v, glm4v_moe, glm4v_moe_text, glm4v_text, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gpt_oss, gptj, gptsan-japanese, granite, granite_speech, granitemoe, granitemoehybrid, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hgnet_v2, hiera, hubert, hunyuan_v1_dense, hunyuan_v1_moe, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, internvl, internvl_vision, jamba, janus, jetmoe, jukebox, kosmos-2, kosmos-2.5, kyutai_speech_to_text, layoutlm, layoutlmv2, layoutlmv3, led, levit, lfm2, lfm2_vl, lightglue, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longcat_flash, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, metaclip_2, mgp-str, mimi, minimax, ministral, mistral, mistral3, mixtral, mlcd, mllama, mm-grounding-dino, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, modernbert-decoder, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmo3, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, ovis2, owlv2, owlvit, paligemma, parakeet, parakeet_ctc, parakeet_encoder, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, perception_encoder, perception_lm, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_omni, qwen2_5_vl, qwen2_5_vl_text, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen2_vl_text, qwen3, qwen3_moe, qwen3_next, qwen3_omni_moe, qwen3_vl, qwen3_vl_moe, qwen3_vl_moe_text, qwen3_vl_text, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam2, sam2_hiera_det_model, sam2_video, sam2_vision_model, sam_hq, sam_hq_vision_model, sam_vision_model, seamless_m4t, seamless_m4t_v2, seed_oss, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip2_vision_model, siglip_vision_model, smollm3, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, t5gemma, table-transformer, tapas, textnet, time_series_transformer, timesfm, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, vaultgemma, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, vjepa2, voxtral, voxtral_encoder, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xcodec, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xlstm, xmod, yolos, yoso, zamba, zamba2, zoedepth"
     ]
    }
   ],
   "source": [
    "sentencepiece_tokenizer = load_tokenizer(os.path.join(TOKENIZERS_PATH, \"sentencepiece\"))\n",
    "\n",
    "text = \"To jest przykład tekstu w języku polskim.\"\n",
    "encoded = sentencepiece_tokenizer.encode(text)\n",
    "\n",
    "print(f\"VocabSize: {sentencepiece_tokenizer.vocab_size}\")\n",
    "\n",
    "for token in encoded:\n",
    "    print(f\"Token: {token}, decoded: {sentencepiece_tokenizer.decode(token)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71c1e67",
   "metadata": {},
   "source": [
    "### Whitespace tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77af2e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhitespaceTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab=None,\n",
    "        unk_token=\"<UNK>\",\n",
    "        pad_token=\"<PAD>\",\n",
    "        bos_token=\"<BOS>\",\n",
    "        eos_token=\"<EOS>\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.vocab = vocab or {}\n",
    "        self.ids_to_tokens = {v: k for k, v in self.vocab.items()}\n",
    "        self.unk_token = unk_token\n",
    "        self.pad_token = pad_token\n",
    "        self.bos_token = bos_token\n",
    "        self.eos_token = eos_token\n",
    "        super().__init__(\n",
    "            unk_token=unk_token,\n",
    "            pad_token=pad_token,\n",
    "            bos_token=bos_token,\n",
    "            eos_token=eos_token,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def encode(self, text, add_special_tokens=False):\n",
    "        tokens = self._tokenize(text)\n",
    "        ids = [self._convert_token_to_id(tok) for tok in tokens]\n",
    "        if add_special_tokens:\n",
    "            ids = [self.vocab[self.bos_token]] + ids + [self.vocab[self.eos_token]]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, token_ids, skip_special_tokens=True):\n",
    "        if isinstance(token_ids, int):\n",
    "            token_ids = [token_ids]\n",
    "        tokens = [self._convert_id_to_token(i) for i in token_ids]\n",
    "        if skip_special_tokens:\n",
    "            tokens = [\n",
    "                t\n",
    "                for t in tokens\n",
    "                if t\n",
    "                not in [self.pad_token, self.unk_token, self.bos_token, self.eos_token]\n",
    "            ]\n",
    "        return self.convert_tokens_to_string(tokens)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "    @property\n",
    "    def all_special_tokens(self):\n",
    "        return [self.pad_token, self.unk_token, self.bos_token, self.eos_token]\n",
    "\n",
    "    @property\n",
    "    def all_special_ids(self):\n",
    "        return [\n",
    "            self.vocab[self.pad_token],\n",
    "            self.vocab[self.unk_token],\n",
    "            self.vocab[self.bos_token],\n",
    "            self.vocab[self.eos_token],\n",
    "        ]\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self.vocab\n",
    "\n",
    "    def _tokenize(self, text: str):\n",
    "        words = text.split()\n",
    "        tokens = []\n",
    "        for word in words:\n",
    "            tokens.extend(re.findall(r'\\w+|[.,!?;:()\"\\']', word))\n",
    "        return tokens\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        return self.vocab.get(token, self.vocab.get(self.unk_token))\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        return self.ids_to_tokens.get(index, self.unk_token)\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def build_vocab(self, texts, vocab_size=50000):\n",
    "        counter = Counter()\n",
    "        for text in texts:\n",
    "            tokens = self._tokenize(text)\n",
    "            counter.update(tokens)\n",
    "\n",
    "        most_common = [t for t, _ in counter.most_common(vocab_size - 4)]\n",
    "\n",
    "        special_tokens = [\n",
    "            self.pad_token,\n",
    "            self.unk_token,\n",
    "            self.bos_token,\n",
    "            self.eos_token,\n",
    "        ]\n",
    "\n",
    "        vocab = {}\n",
    "\n",
    "        for i, tok in enumerate(special_tokens):\n",
    "            vocab[tok] = i\n",
    "\n",
    "        for i, tok in enumerate(most_common, start=len(special_tokens)):\n",
    "            vocab[tok] = i\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.ids_to_tokens = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    def save_pretrained(self, save_directory):\n",
    "        vocab_file = f\"{save_directory}/vocab.json\"\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.vocab, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Saved vocabulary to {vocab_file}\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, load_directory):\n",
    "        vocab_file = f\"{load_directory}/vocab.json\"\n",
    "        with open(vocab_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            vocab = json.load(f)\n",
    "        print(f\"Loaded vocabulary from {vocab_file}\")\n",
    "        return cls(vocab=vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57c89a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved vocabulary to ./tokenizers/whitespace/vocab.json\n"
     ]
    }
   ],
   "source": [
    "whitespace_tokenizer = WhitespaceTokenizer()\n",
    "whitespace_tokenizer.build_vocab([train_text], vocab_size=vocab_size)\n",
    "\n",
    "whitespace_tokenizer.save_pretrained(\n",
    "    save_directory=os.path.join(TOKENIZERS_PATH, \"whitespace\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09db3221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocabulary from ./tokenizers/whitespace/vocab.json\n",
      "Tokenizer loaded from: ./tokenizers/whitespace\n",
      "VocabSize: 52000\n",
      "Token: 81, decoded: To\n",
      "Token: 23, decoded: jest\n",
      "Token: 659, decoded: przykład\n",
      "Token: 10864, decoded: tekstu\n",
      "Token: 8, decoded: w\n",
      "Token: 3264, decoded: języku\n",
      "Token: 5100, decoded: polskim\n",
      "Token: 5, decoded: .\n"
     ]
    }
   ],
   "source": [
    "whitespace_tokenizer = load_tokenizer(\n",
    "    os.path.join(TOKENIZERS_PATH, \"whitespace\"), custom_class=WhitespaceTokenizer\n",
    ")\n",
    "\n",
    "text = \"To jest przykład tekstu w języku polskim.\"\n",
    "encoded = whitespace_tokenizer.encode(text)\n",
    "\n",
    "print(f\"VocabSize: {whitespace_tokenizer.vocab_size}\")\n",
    "\n",
    "for token in encoded:\n",
    "    print(f\"Token: {token}, decoded: {whitespace_tokenizer.decode(token)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b02dbdf",
   "metadata": {},
   "source": [
    "## Creating torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7919fc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) // self.seq_len - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = idx * self.seq_len\n",
    "        x = self.data[i : i + self.seq_len]\n",
    "        y = self.data[i + 1 : i + 1 + self.seq_len]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def create_tensor(docs: List[str], tokenizer):\n",
    "    tokenized_docs = []\n",
    "    sep = tokenizer.sep_token\n",
    "\n",
    "    counter = 0\n",
    "    for doc in docs:\n",
    "        tokens = tokenizer.encode(doc + sep)\n",
    "        tokenized_docs.extend(tokens)\n",
    "\n",
    "        if counter % 1000 == 0:\n",
    "            print(f\"Parsed: {counter}/{len(docs)} docs\")\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    data = torch.tensor(tokenized_docs, dtype=torch.long)\n",
    "    print(f\"Data shape: {data.shape}\")\n",
    "    return data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computional-linguistic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
