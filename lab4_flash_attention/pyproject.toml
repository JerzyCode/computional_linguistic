[project]
name = "lab4-flash-attention"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = "==3.10"
dependencies = [
    "torch==2.5.1",
    "transformers>=4.30.0",
    "flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3+cu124torch2.5-cp310-cp310-linux_x86_64.whl",
    "tqdm>=4.64.0",
    "jupyter",
    "markupsafe<3.0",
    "zstandard>=0.22.0",
]