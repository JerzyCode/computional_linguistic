{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5592c318",
   "metadata": {},
   "source": [
    "## Notebook for models\n",
    "\n",
    "Overall plan is to train 4 models:\n",
    "- SVM on top of the herbert-large-uncased\n",
    "- From scratch model enkoder-only with size similar herbert-large\n",
    "- Finetunning on herbert-large with add of classification head to the model\n",
    "- Finetunning herber large: contrastive learning on the meaning -> classification finetune "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2073bf23",
   "metadata": {},
   "source": [
    "### Imports & setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0e9bf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76283ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 26 19:51:21 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:87:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             54W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a00d1f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_HOME=\"/net/tscratch/people/plgboksa/lab3_computional_ling_better_final/model\"\n",
    "!export HF_DATASETS_CACHE=\"/net/tscratch/people/plgboksa/lab3_computional_ling_better_final/cache\"\n",
    "!export HF_METRICS_CACHE=\"/net/tscratch/people/plgboksa/lab3_computional_ling_better_final/cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0386b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"allegro/herbert-large-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-large-cased\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e6d42e",
   "metadata": {},
   "source": [
    "### Datasets preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d791363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlangClassificationDataset(Dataset):\n",
    "    def __init__(self, path, tokenizer, max_len=128):\n",
    "        self.df = pd.read_csv(path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        text = row[\"text\"]\n",
    "        label = row[\"label\"]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "963c8132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 4336\n",
      "Validation dataset size: 542\n"
     ]
    }
   ],
   "source": [
    "train_datast = SlangClassificationDataset(\"prepared_data/train.csv\", tokenizer)\n",
    "val_datast = SlangClassificationDataset(\"prepared_data/val.csv\", tokenizer)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_datast)}\")\n",
    "print(f\"Validation dataset size: {len(val_datast)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dec7caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SlangClassificationDataset(\"prepared_data/test.csv\", tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54b67a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_model(model, dataset, batch_size):\n",
    "    model.eval()\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            probs = model.forward(input_ids, attention_mask)\n",
    "            y_pred = torch.argmax(probs, dim=1)\n",
    "\n",
    "            all_preds.append(y_pred.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    y_pred_cpu = torch.cat(all_preds).numpy()\n",
    "    y_true = torch.cat(all_labels).numpy()\n",
    "\n",
    "    return {\n",
    "        \"f1\": f1_score(y_true, y_pred_cpu, average=\"weighted\"),\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred_cpu),\n",
    "    }\n",
    "\n",
    "\n",
    "def inference_with_timing(model, dataloader, device=None, desc=\"Inference\"):\n",
    "    all_preds = []\n",
    "    batch_times = []\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=desc, unit=\"batch\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        start_batch = time.time()\n",
    "        probs = model.forward(input_ids, attention_mask)\n",
    "        y_pred = torch.argmax(probs, dim=1)\n",
    "        end_batch = time.time()\n",
    "\n",
    "        batch_times.append(end_batch - start_batch)\n",
    "        all_preds.append(y_pred.cpu())\n",
    "\n",
    "    avg_batch_time = sum(batch_times) / len(batch_times)\n",
    "\n",
    "    y_pred_all = torch.cat(all_preds)\n",
    "    return y_pred_all, avg_batch_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0f1f66",
   "metadata": {},
   "source": [
    "### 1. HerBERTSVM\n",
    "\n",
    "Attached SVM on top of the Herbert embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fe39fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HerBERTSVM(nn.Module):\n",
    "    def __init__(\n",
    "        self, model_name=\"allegro/herbert-large-cased\", svm_kernel=\"linear\", svm_C=1.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.clf = SVC(kernel=svm_kernel, C=svm_C, probability=True)\n",
    "\n",
    "    def encode_texts(self, input_ids, attention_mask, batch_size=16) -> np.ndarray:\n",
    "        all_embeddings = []\n",
    "\n",
    "        for i in range(0, input_ids.size(0), batch_size):\n",
    "            batch_input_ids = input_ids[i : i + batch_size].to(self.device)\n",
    "            batch_attention_mask = attention_mask[i : i + batch_size].to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(\n",
    "                    input_ids=batch_input_ids, attention_mask=batch_attention_mask\n",
    "                )\n",
    "                cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "            all_embeddings.append(cls_embeddings.cpu().numpy())\n",
    "\n",
    "        return np.vstack(all_embeddings)\n",
    "\n",
    "    def predict(self, input_ids, attention_mask) -> np.ndarray:\n",
    "        X = self.encode_texts(input_ids, attention_mask)\n",
    "        return self.clf.predict(X)\n",
    "\n",
    "    def predict_proba(self, input_ids, attention_mask) -> torch.Tensor:\n",
    "        X = self.encode_texts(input_ids, attention_mask)\n",
    "        probs = self.clf.predict_proba(X)\n",
    "        return torch.tensor(probs, dtype=torch.float32)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, batch_size=16) -> torch.Tensor:\n",
    "        return self.predict_proba(input_ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "816e79aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allegro/herbert-large-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "herbert_svm = HerBERTSVM()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417429ac",
   "metadata": {},
   "source": [
    "### HerBERTSVM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac04e0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training HerBERT + SVM model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting CLS embeddings: 100%|████████████████████████████████████████████████████████████████████████████████████| 271/271 [00:23<00:00, 11.30batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start SVM Fit\n",
      "Training lasts for: 62.30 seconds\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "train_loader = DataLoader(train_datast, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "all_embeddings = []\n",
    "all_labels = []\n",
    "\n",
    "\n",
    "print(\"Start training HerBERT + SVM model\")\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(train_loader, desc=\"Extracting CLS embeddings\", unit=\"batch\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        outputs = herbert_svm.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        all_embeddings.append(cls_embeddings.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "\n",
    "X_train = np.vstack(all_embeddings)\n",
    "y_train = np.concatenate(all_labels)\n",
    "\n",
    "print(\"Start SVM Fit\")\n",
    "herbert_svm.clf.fit(X_train, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training lasts for: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c16f819",
   "metadata": {},
   "source": [
    "### Test SVM on evaluation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25a657e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Validation F1 Score: 0.6491\n",
      "SVM Validation Accuracy: 0.6605\n"
     ]
    }
   ],
   "source": [
    "svm_metrics = verify_model(herbert_svm, val_datast, batch_size=16)\n",
    "print(f\"SVM Validation F1 Score: {svm_metrics['f1']:.4f}\")\n",
    "print(f\"SVM Validation Accuracy: {svm_metrics['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58b8185",
   "metadata": {},
   "source": [
    "SVM was trained for 62.30s with embedding creation.\n",
    "\n",
    "It was able to reach accuracy of 0.6605 and f1_Score of 0.6491.\n",
    "\n",
    "Not bad, not bad.\n",
    "\n",
    "Let's test inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9575daf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "test_svm_text = \"bambik z ciebie jest leszczu\"\n",
    "\n",
    "tok = herbert_svm.tokenizer(\n",
    "    test_svm_text,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "input_ids = tok[\"input_ids\"]\n",
    "attention_mask = tok[\"attention_mask\"]\n",
    "\n",
    "probs = herbert_svm.forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "print(torch.argmax(probs, dim=1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe03c48",
   "metadata": {},
   "source": [
    "### Test data evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "142fbfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Test F1 Score: 0.6198\n",
      "SVM Test Accuracy: 0.6354\n"
     ]
    }
   ],
   "source": [
    "svm_metrics_test = verify_model(herbert_svm, test_dataset, batch_size=16)\n",
    "print(f\"SVM Test F1 Score: {svm_metrics_test['f1']:.4f}\")\n",
    "print(f\"SVM Test Accuracy: {svm_metrics_test['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c2cebb",
   "metadata": {},
   "source": [
    "**inference_time** for batch of size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1040204f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:03<00:00,  8.94batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per batch: 0.1030 seconds\n",
      "**inference_time** for batch of size 16: 0.1030 seconds\n",
      "Test accuracy: 0.6354\n",
      "Test f1: 0.6198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_all, avg_batch_time = inference_with_timing(\n",
    "    herbert_svm, test_loader, device=device\n",
    ")\n",
    "\n",
    "print(f\"**inference_time** for batch of size 16: {avg_batch_time:.4f} seconds\")\n",
    "print(\n",
    "    f\"Test accuracy: {accuracy_score(test_dataset.df['label'], y_pred_all.numpy()):.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Test f1: {f1_score(test_dataset.df['label'], y_pred_all.numpy(), average='weighted'):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe22477",
   "metadata": {},
   "source": [
    "**summary for herbert_svm**\n",
    "\n",
    "Training time: 62.30s\n",
    "\n",
    "Average inference_time per batch of size 16: 0.1030 seconds\n",
    "\n",
    "Test accuracy: 0.6354\n",
    "\n",
    "Test f1: 0,6198"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237cab94",
   "metadata": {},
   "source": [
    "Let's move to the neural networks for the task\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba540cf2",
   "metadata": {},
   "source": [
    "### Neural networks helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b065591",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingMetrics:\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "        self.val_f1s = []\n",
    "\n",
    "    def insert(self, train_loss, val_loss, val_accuracy, val_f1):\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.val_accuracies.append(val_accuracy)\n",
    "        self.val_f1s.append(val_f1)\n",
    "\n",
    "    def to_pd(self):\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                \"train_loss\": self.train_losses,\n",
    "                \"val_loss\": self.val_losses,\n",
    "                \"val_accuracy\": self.val_accuracies,\n",
    "                \"val_f1\": self.val_f1s,\n",
    "            }\n",
    "        )\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc2651ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, val_loader: DataLoader, loss_fn) -> dict:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            y_pred = torch.argmax(outputs, dim=1)\n",
    "            all_preds.append(y_pred.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    y_pred_cpu = torch.cat(all_preds).numpy()\n",
    "    y_true = torch.cat(all_labels).numpy()\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred_cpu)\n",
    "    f1 = f1_score(y_true, y_pred_cpu, average=\"weighted\")\n",
    "\n",
    "    return {\n",
    "        \"val_loss\": avg_loss,\n",
    "        \"val_accuracy\": accuracy,\n",
    "        \"val_f1\": f1,\n",
    "    }\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    train_dataset: Dataset,\n",
    "    val_dataset: Dataset,\n",
    "    optimizer: torch.optim.Optimizer = None,\n",
    "    epochs: int = 3,\n",
    "    batch_size: int = 16,\n",
    "    lr: float = 1e-4,\n",
    "    log_freq: int = 10,\n",
    "):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    if optimizer is None:\n",
    "        optimizer = AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    metrics = TrainingMetrics()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            if (batch_idx + 1) % log_freq == 0:\n",
    "                avg_train_loss = total_train_loss / log_freq\n",
    "                total_train_loss = 0.0\n",
    "\n",
    "                eval_metrics = evaluate(model, val_loader, loss_fn)\n",
    "                print(\n",
    "                    f\"Batch {batch_idx + 1} - Avg Train Loss: {avg_train_loss:.4f} \"\n",
    "                    f\"Validation Loss: {eval_metrics['val_loss']:.4f}, \"\n",
    "                    f\"Accuracy: {eval_metrics['val_accuracy']:.4f}, \"\n",
    "                    f\"F1 Score: {eval_metrics['val_f1']:.4f}\"\n",
    "                )\n",
    "                metrics.insert(\n",
    "                    avg_train_loss,\n",
    "                    eval_metrics[\"val_loss\"],\n",
    "                    eval_metrics[\"val_accuracy\"],\n",
    "                    eval_metrics[\"val_f1\"],\n",
    "                )\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} completed, training metrics recorded.\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d72f68",
   "metadata": {},
   "source": [
    "### Training enkoder-only model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5967fc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embed_dim=1024,\n",
    "        num_heads=16,\n",
    "        hidden_dim=4096,\n",
    "        num_layers=24,\n",
    "        max_len=128,\n",
    "        num_labels=3,\n",
    "        dropout=0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, embed_dim))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation=\"gelu\",\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, num_layers=num_layers\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(embed_dim, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = x + self.pos_embedding[:, : x.size(1), :]\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            src_key_padding_mask = ~attention_mask.bool()\n",
    "        else:\n",
    "            src_key_padding_mask = None\n",
    "\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        cls_token = x[:, 0, :]\n",
    "        cls_token = self.dropout(cls_token)\n",
    "\n",
    "        logits = self.classifier(cls_token)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f1f672e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 353,643,523 parameters\n"
     ]
    }
   ],
   "source": [
    "model = ScratchClassifier(vocab_size=tokenizer.vocab_size, num_labels=3)\n",
    "model.to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model has {total_params:,} parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55f6fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10 - Avg Train Loss: 1.1797 Validation Loss: 1.0338, Accuracy: 0.5000, F1 Score: 0.3521\n",
      "Batch 20 - Avg Train Loss: 0.9945 Validation Loss: 1.0645, Accuracy: 0.3930, F1 Score: 0.3478\n",
      "Batch 30 - Avg Train Loss: 1.0398 Validation Loss: 1.1414, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 40 - Avg Train Loss: 1.0603 Validation Loss: 1.0252, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 50 - Avg Train Loss: 1.1085 Validation Loss: 1.0558, Accuracy: 0.4742, F1 Score: 0.3994\n",
      "Batch 60 - Avg Train Loss: 1.0511 Validation Loss: 1.0368, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 70 - Avg Train Loss: 1.0546 Validation Loss: 1.0221, Accuracy: 0.5074, F1 Score: 0.4170\n",
      "Batch 80 - Avg Train Loss: 1.0815 Validation Loss: 1.0001, Accuracy: 0.5258, F1 Score: 0.3872\n",
      "Batch 90 - Avg Train Loss: 1.0435 Validation Loss: 1.0295, Accuracy: 0.3469, F1 Score: 0.2632\n",
      "Batch 100 - Avg Train Loss: 1.0424 Validation Loss: 1.0174, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 110 - Avg Train Loss: 0.9419 Validation Loss: 1.0043, Accuracy: 0.5092, F1 Score: 0.3495\n",
      "Batch 120 - Avg Train Loss: 0.9906 Validation Loss: 0.9871, Accuracy: 0.5129, F1 Score: 0.4377\n",
      "Batch 130 - Avg Train Loss: 0.9531 Validation Loss: 0.9877, Accuracy: 0.5295, F1 Score: 0.4363\n",
      "Batch 140 - Avg Train Loss: 1.0634 Validation Loss: 1.0281, Accuracy: 0.4871, F1 Score: 0.4220\n",
      "Batch 150 - Avg Train Loss: 0.9467 Validation Loss: 0.9802, Accuracy: 0.5295, F1 Score: 0.4309\n",
      "Batch 160 - Avg Train Loss: 1.0050 Validation Loss: 0.9956, Accuracy: 0.5148, F1 Score: 0.4371\n",
      "Batch 170 - Avg Train Loss: 1.0672 Validation Loss: 0.9624, Accuracy: 0.5295, F1 Score: 0.3994\n",
      "Batch 180 - Avg Train Loss: 1.0778 Validation Loss: 1.0176, Accuracy: 0.5535, F1 Score: 0.4581\n",
      "Batch 190 - Avg Train Loss: 1.0568 Validation Loss: 0.9869, Accuracy: 0.5185, F1 Score: 0.3711\n",
      "Batch 200 - Avg Train Loss: 1.0089 Validation Loss: 0.9773, Accuracy: 0.5240, F1 Score: 0.3861\n",
      "Batch 210 - Avg Train Loss: 0.9283 Validation Loss: 0.9459, Accuracy: 0.5443, F1 Score: 0.4315\n",
      "Batch 220 - Avg Train Loss: 0.9887 Validation Loss: 0.9947, Accuracy: 0.5055, F1 Score: 0.4691\n",
      "Batch 230 - Avg Train Loss: 1.0052 Validation Loss: 0.9797, Accuracy: 0.5148, F1 Score: 0.3804\n",
      "Batch 240 - Avg Train Loss: 0.9500 Validation Loss: 0.9551, Accuracy: 0.5461, F1 Score: 0.4515\n",
      "Batch 250 - Avg Train Loss: 0.9352 Validation Loss: 0.9483, Accuracy: 0.5443, F1 Score: 0.4739\n",
      "Batch 260 - Avg Train Loss: 0.9750 Validation Loss: 0.9484, Accuracy: 0.5517, F1 Score: 0.4551\n",
      "Batch 270 - Avg Train Loss: 0.9780 Validation Loss: 1.0105, Accuracy: 0.4871, F1 Score: 0.4139\n",
      "Epoch 1 completed, training metrics recorded.\n",
      "Batch 10 - Avg Train Loss: 0.9813 Validation Loss: 1.0008, Accuracy: 0.5111, F1 Score: 0.4275\n",
      "Batch 20 - Avg Train Loss: 0.9375 Validation Loss: 0.9564, Accuracy: 0.5498, F1 Score: 0.4444\n",
      "Batch 30 - Avg Train Loss: 0.9254 Validation Loss: 0.9649, Accuracy: 0.5535, F1 Score: 0.4498\n",
      "Batch 40 - Avg Train Loss: 0.9688 Validation Loss: 1.0135, Accuracy: 0.5203, F1 Score: 0.3746\n",
      "Batch 50 - Avg Train Loss: 0.9865 Validation Loss: 1.0131, Accuracy: 0.4077, F1 Score: 0.3725\n",
      "Batch 60 - Avg Train Loss: 0.9860 Validation Loss: 0.9454, Accuracy: 0.5517, F1 Score: 0.4520\n",
      "Batch 70 - Avg Train Loss: 1.0055 Validation Loss: 0.9685, Accuracy: 0.5369, F1 Score: 0.5003\n",
      "Batch 80 - Avg Train Loss: 0.9703 Validation Loss: 0.9803, Accuracy: 0.5627, F1 Score: 0.4566\n",
      "Batch 90 - Avg Train Loss: 0.8885 Validation Loss: 0.9705, Accuracy: 0.5387, F1 Score: 0.4718\n",
      "Batch 100 - Avg Train Loss: 0.8635 Validation Loss: 0.9732, Accuracy: 0.5535, F1 Score: 0.4896\n",
      "Batch 110 - Avg Train Loss: 1.0327 Validation Loss: 1.0570, Accuracy: 0.5037, F1 Score: 0.4498\n",
      "Batch 120 - Avg Train Loss: 0.9589 Validation Loss: 0.9711, Accuracy: 0.5295, F1 Score: 0.4010\n",
      "Batch 130 - Avg Train Loss: 1.0054 Validation Loss: 0.9351, Accuracy: 0.5812, F1 Score: 0.5417\n",
      "Batch 140 - Avg Train Loss: 0.9034 Validation Loss: 0.9494, Accuracy: 0.5461, F1 Score: 0.4777\n",
      "Batch 150 - Avg Train Loss: 0.9301 Validation Loss: 0.9484, Accuracy: 0.5277, F1 Score: 0.4893\n",
      "Batch 160 - Avg Train Loss: 1.0147 Validation Loss: 0.9421, Accuracy: 0.5480, F1 Score: 0.4462\n",
      "Batch 170 - Avg Train Loss: 0.9126 Validation Loss: 0.9544, Accuracy: 0.5480, F1 Score: 0.4773\n",
      "Batch 180 - Avg Train Loss: 0.9224 Validation Loss: 0.9484, Accuracy: 0.5387, F1 Score: 0.4718\n",
      "Batch 190 - Avg Train Loss: 0.9051 Validation Loss: 0.9382, Accuracy: 0.5480, F1 Score: 0.4375\n",
      "Batch 200 - Avg Train Loss: 0.8984 Validation Loss: 1.0117, Accuracy: 0.4963, F1 Score: 0.4215\n",
      "Batch 210 - Avg Train Loss: 0.8583 Validation Loss: 0.9760, Accuracy: 0.5480, F1 Score: 0.4669\n",
      "Batch 220 - Avg Train Loss: 0.8895 Validation Loss: 1.0014, Accuracy: 0.4723, F1 Score: 0.4807\n",
      "Batch 230 - Avg Train Loss: 0.9388 Validation Loss: 0.9680, Accuracy: 0.5498, F1 Score: 0.4457\n",
      "Batch 240 - Avg Train Loss: 0.9541 Validation Loss: 0.9356, Accuracy: 0.5554, F1 Score: 0.4461\n",
      "Batch 250 - Avg Train Loss: 0.9084 Validation Loss: 0.9846, Accuracy: 0.5535, F1 Score: 0.4408\n",
      "Batch 260 - Avg Train Loss: 0.9202 Validation Loss: 0.9513, Accuracy: 0.5480, F1 Score: 0.4727\n",
      "Batch 270 - Avg Train Loss: 0.9485 Validation Loss: 1.0029, Accuracy: 0.4815, F1 Score: 0.4459\n",
      "Epoch 2 completed, training metrics recorded.\n",
      "Batch 10 - Avg Train Loss: 0.8432 Validation Loss: 0.9725, Accuracy: 0.5461, F1 Score: 0.4603\n",
      "Batch 20 - Avg Train Loss: 0.8279 Validation Loss: 0.9918, Accuracy: 0.5332, F1 Score: 0.5283\n",
      "Batch 30 - Avg Train Loss: 0.9061 Validation Loss: 0.9561, Accuracy: 0.5627, F1 Score: 0.4486\n",
      "Batch 40 - Avg Train Loss: 0.8825 Validation Loss: 0.9319, Accuracy: 0.5701, F1 Score: 0.5443\n",
      "Batch 50 - Avg Train Loss: 0.9035 Validation Loss: 0.9878, Accuracy: 0.5535, F1 Score: 0.4642\n",
      "Batch 60 - Avg Train Loss: 0.8600 Validation Loss: 0.9505, Accuracy: 0.5756, F1 Score: 0.5332\n",
      "Batch 70 - Avg Train Loss: 0.8870 Validation Loss: 0.9357, Accuracy: 0.5554, F1 Score: 0.5006\n",
      "Batch 80 - Avg Train Loss: 0.8371 Validation Loss: 0.9874, Accuracy: 0.5258, F1 Score: 0.5216\n",
      "Batch 90 - Avg Train Loss: 0.8874 Validation Loss: 0.9203, Accuracy: 0.5738, F1 Score: 0.4807\n",
      "Batch 100 - Avg Train Loss: 0.7899 Validation Loss: 1.0176, Accuracy: 0.5627, F1 Score: 0.4646\n",
      "Batch 110 - Avg Train Loss: 0.8727 Validation Loss: 1.0743, Accuracy: 0.4373, F1 Score: 0.4162\n",
      "Batch 120 - Avg Train Loss: 0.9662 Validation Loss: 0.9419, Accuracy: 0.5461, F1 Score: 0.4402\n",
      "Batch 130 - Avg Train Loss: 0.9139 Validation Loss: 0.9273, Accuracy: 0.5664, F1 Score: 0.5077\n",
      "Batch 140 - Avg Train Loss: 0.8631 Validation Loss: 0.9580, Accuracy: 0.5406, F1 Score: 0.5463\n",
      "Batch 150 - Avg Train Loss: 0.8285 Validation Loss: 0.9665, Accuracy: 0.5535, F1 Score: 0.4804\n",
      "Batch 160 - Avg Train Loss: 0.8589 Validation Loss: 0.9411, Accuracy: 0.5627, F1 Score: 0.5238\n",
      "Batch 170 - Avg Train Loss: 0.7971 Validation Loss: 0.9345, Accuracy: 0.5572, F1 Score: 0.5116\n",
      "Batch 180 - Avg Train Loss: 0.8060 Validation Loss: 1.0439, Accuracy: 0.5240, F1 Score: 0.4671\n",
      "Batch 190 - Avg Train Loss: 0.9781 Validation Loss: 1.0493, Accuracy: 0.5111, F1 Score: 0.3670\n",
      "Batch 200 - Avg Train Loss: 0.9203 Validation Loss: 0.9768, Accuracy: 0.5055, F1 Score: 0.3415\n",
      "Batch 210 - Avg Train Loss: 1.0009 Validation Loss: 0.9656, Accuracy: 0.5221, F1 Score: 0.4171\n",
      "Batch 220 - Avg Train Loss: 0.8820 Validation Loss: 1.0617, Accuracy: 0.5387, F1 Score: 0.4323\n",
      "Batch 230 - Avg Train Loss: 0.9738 Validation Loss: 0.9688, Accuracy: 0.5683, F1 Score: 0.4640\n",
      "Batch 240 - Avg Train Loss: 0.9795 Validation Loss: 0.9384, Accuracy: 0.5701, F1 Score: 0.5448\n",
      "Batch 250 - Avg Train Loss: 0.9361 Validation Loss: 0.9275, Accuracy: 0.5627, F1 Score: 0.5017\n",
      "Batch 260 - Avg Train Loss: 0.8762 Validation Loss: 0.9301, Accuracy: 0.5701, F1 Score: 0.5308\n",
      "Batch 270 - Avg Train Loss: 0.8343 Validation Loss: 0.9706, Accuracy: 0.4982, F1 Score: 0.5036\n",
      "Epoch 3 completed, training metrics recorded.\n",
      "Total training time: 325.61 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "training_metrics = train(\n",
    "    model, train_datast, val_datast, epochs=3, batch_size=16, lr=1e-5, log_freq=10\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "total_training_time = end_time - start_time\n",
    "print(f\"Total training time: {total_training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4f6d0631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:03<00:00,  9.89batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**inference_time** for batch of size 16: 0.0216 seconds\n",
      "Test accuracy: 0.5230\n",
      "Test f1: 0.5289\n"
     ]
    }
   ],
   "source": [
    "training_metrics.to_pd().to_csv(\n",
    "    \"results/scratch_model_training_metrics.csv\", index=False\n",
    ")\n",
    "\n",
    "y_pred_all, avg_batch_time = inference_with_timing(model, test_loader, device=device)\n",
    "\n",
    "print(f\"**inference_time** for batch of size 16: {avg_batch_time:.4f} seconds\")\n",
    "print(\n",
    "    f\"Test accuracy: {accuracy_score(test_dataset.df['label'], y_pred_all.numpy()):.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Test f1: {f1_score(test_dataset.df['label'], y_pred_all.numpy(), average='weighted'):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ea5501",
   "metadata": {},
   "source": [
    "**summary for scratch**\n",
    "\n",
    "Training time: 325.61s\n",
    "\n",
    "Average inference_time per batch of size 16: 0.0216 seconds\n",
    "\n",
    "Test accuracy: 0.5230\n",
    "\n",
    "Test f1: 0.5289"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3d9ab0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "test_text = \"bambik z ciebie jest leszczu\"\n",
    "\n",
    "tok = tokenizer(\n",
    "    test_text,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "input_ids = tok[\"input_ids\"]\n",
    "attention_mask = tok[\"attention_mask\"]\n",
    "\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "model.to(device)\n",
    "\n",
    "probs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "print(torch.argmax(probs, dim=1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6b7dc6",
   "metadata": {},
   "source": [
    "### Finetuning model for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cf074ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinetuneModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"allegro/herbert-large-cased\",\n",
    "        num_labels=3,\n",
    "        dropout=0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name, device_map=None)\n",
    "        hidden_size = self.model.config.hidden_size\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for layer_idx in range(8, 12):\n",
    "            for param in self.model.encoder.layer[layer_idx].parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        if isinstance(out, tuple):\n",
    "            hidden_state = out[0]\n",
    "        else:\n",
    "            hidden_state = out.last_hidden_state\n",
    "\n",
    "        cls = hidden_state[:, 0, :]\n",
    "        x = self.dropout(cls)\n",
    "        logits = self.classifier(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec1ac60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allegro/herbert-large-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FinetuneModel(\n",
       "  (model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(50000, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=3, bias=True)\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FinetuneModel()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34db1677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10 - Avg Train Loss: 1.1202 Validation Loss: 1.0708, Accuracy: 0.5018, F1 Score: 0.3402\n",
      "Batch 20 - Avg Train Loss: 1.0616 Validation Loss: 1.0460, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 30 - Avg Train Loss: 1.0164 Validation Loss: 1.0293, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 40 - Avg Train Loss: 1.0127 Validation Loss: 1.0220, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 50 - Avg Train Loss: 0.9702 Validation Loss: 1.0194, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 60 - Avg Train Loss: 1.0519 Validation Loss: 1.0161, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 70 - Avg Train Loss: 1.0277 Validation Loss: 1.0138, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 80 - Avg Train Loss: 0.9656 Validation Loss: 1.0109, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 90 - Avg Train Loss: 0.9662 Validation Loss: 1.0086, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 100 - Avg Train Loss: 1.0041 Validation Loss: 1.0042, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 110 - Avg Train Loss: 0.9892 Validation Loss: 0.9985, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 120 - Avg Train Loss: 0.9561 Validation Loss: 0.9948, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 130 - Avg Train Loss: 0.9991 Validation Loss: 0.9913, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 140 - Avg Train Loss: 0.9762 Validation Loss: 0.9890, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 150 - Avg Train Loss: 0.9607 Validation Loss: 0.9865, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 160 - Avg Train Loss: 0.9955 Validation Loss: 0.9834, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 170 - Avg Train Loss: 0.9917 Validation Loss: 0.9793, Accuracy: 0.5037, F1 Score: 0.3407\n",
      "Batch 180 - Avg Train Loss: 0.9361 Validation Loss: 0.9770, Accuracy: 0.5037, F1 Score: 0.3407\n",
      "Batch 190 - Avg Train Loss: 0.9487 Validation Loss: 0.9746, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 200 - Avg Train Loss: 0.9481 Validation Loss: 0.9707, Accuracy: 0.5018, F1 Score: 0.3366\n",
      "Batch 210 - Avg Train Loss: 0.9494 Validation Loss: 0.9632, Accuracy: 0.5055, F1 Score: 0.3536\n",
      "Batch 220 - Avg Train Loss: 0.9248 Validation Loss: 0.9609, Accuracy: 0.5129, F1 Score: 0.3801\n",
      "Batch 230 - Avg Train Loss: 0.9405 Validation Loss: 0.9566, Accuracy: 0.5092, F1 Score: 0.3700\n",
      "Batch 240 - Avg Train Loss: 0.9831 Validation Loss: 0.9517, Accuracy: 0.5074, F1 Score: 0.3614\n",
      "Batch 250 - Avg Train Loss: 0.9306 Validation Loss: 0.9481, Accuracy: 0.5166, F1 Score: 0.3805\n",
      "Batch 260 - Avg Train Loss: 0.9400 Validation Loss: 0.9425, Accuracy: 0.5295, F1 Score: 0.4114\n",
      "Batch 270 - Avg Train Loss: 0.9588 Validation Loss: 0.9381, Accuracy: 0.5554, F1 Score: 0.4670\n",
      "Epoch 1 completed, training metrics recorded.\n",
      "Batch 10 - Avg Train Loss: 0.9473 Validation Loss: 0.9321, Accuracy: 0.5793, F1 Score: 0.5307\n",
      "Batch 20 - Avg Train Loss: 0.9090 Validation Loss: 0.9254, Accuracy: 0.5756, F1 Score: 0.5305\n",
      "Batch 30 - Avg Train Loss: 0.9417 Validation Loss: 0.9181, Accuracy: 0.5904, F1 Score: 0.5490\n",
      "Batch 40 - Avg Train Loss: 0.8656 Validation Loss: 0.9109, Accuracy: 0.5923, F1 Score: 0.5365\n",
      "Batch 50 - Avg Train Loss: 0.9364 Validation Loss: 0.9132, Accuracy: 0.5886, F1 Score: 0.5458\n",
      "Batch 60 - Avg Train Loss: 0.8606 Validation Loss: 0.9125, Accuracy: 0.5701, F1 Score: 0.4956\n",
      "Batch 70 - Avg Train Loss: 0.9085 Validation Loss: 0.9084, Accuracy: 0.5756, F1 Score: 0.4953\n",
      "Batch 80 - Avg Train Loss: 0.8472 Validation Loss: 0.8957, Accuracy: 0.5793, F1 Score: 0.5288\n",
      "Batch 90 - Avg Train Loss: 0.7817 Validation Loss: 0.9008, Accuracy: 0.5701, F1 Score: 0.5342\n",
      "Batch 100 - Avg Train Loss: 0.8300 Validation Loss: 0.8915, Accuracy: 0.5683, F1 Score: 0.5252\n",
      "Batch 110 - Avg Train Loss: 0.8438 Validation Loss: 0.8750, Accuracy: 0.6144, F1 Score: 0.5972\n",
      "Batch 120 - Avg Train Loss: 0.8302 Validation Loss: 0.8685, Accuracy: 0.6107, F1 Score: 0.6012\n",
      "Batch 130 - Avg Train Loss: 0.8092 Validation Loss: 0.8606, Accuracy: 0.6144, F1 Score: 0.5818\n",
      "Batch 140 - Avg Train Loss: 0.7840 Validation Loss: 0.8525, Accuracy: 0.6162, F1 Score: 0.5928\n",
      "Batch 150 - Avg Train Loss: 0.7732 Validation Loss: 0.8635, Accuracy: 0.5978, F1 Score: 0.5771\n",
      "Batch 160 - Avg Train Loss: 0.7564 Validation Loss: 0.8653, Accuracy: 0.5886, F1 Score: 0.5727\n",
      "Batch 170 - Avg Train Loss: 0.7946 Validation Loss: 0.8435, Accuracy: 0.6273, F1 Score: 0.6088\n",
      "Batch 180 - Avg Train Loss: 0.7820 Validation Loss: 0.8380, Accuracy: 0.6218, F1 Score: 0.6132\n",
      "Batch 190 - Avg Train Loss: 0.8142 Validation Loss: 0.8301, Accuracy: 0.6365, F1 Score: 0.6124\n",
      "Batch 200 - Avg Train Loss: 0.7785 Validation Loss: 0.8443, Accuracy: 0.6384, F1 Score: 0.6125\n",
      "Batch 210 - Avg Train Loss: 0.7422 Validation Loss: 0.8246, Accuracy: 0.6273, F1 Score: 0.5986\n",
      "Batch 220 - Avg Train Loss: 0.7176 Validation Loss: 0.8215, Accuracy: 0.6365, F1 Score: 0.6147\n",
      "Batch 230 - Avg Train Loss: 0.7566 Validation Loss: 0.8340, Accuracy: 0.6310, F1 Score: 0.6272\n",
      "Batch 240 - Avg Train Loss: 0.7434 Validation Loss: 0.8196, Accuracy: 0.6458, F1 Score: 0.6287\n",
      "Batch 250 - Avg Train Loss: 0.6794 Validation Loss: 0.8395, Accuracy: 0.6292, F1 Score: 0.5972\n",
      "Batch 260 - Avg Train Loss: 0.9048 Validation Loss: 0.8213, Accuracy: 0.6292, F1 Score: 0.6053\n",
      "Batch 270 - Avg Train Loss: 0.6858 Validation Loss: 0.8085, Accuracy: 0.6402, F1 Score: 0.6351\n",
      "Epoch 2 completed, training metrics recorded.\n",
      "Batch 10 - Avg Train Loss: 0.7508 Validation Loss: 0.8179, Accuracy: 0.6458, F1 Score: 0.6100\n",
      "Batch 20 - Avg Train Loss: 0.6208 Validation Loss: 0.8010, Accuracy: 0.6384, F1 Score: 0.6285\n",
      "Batch 30 - Avg Train Loss: 0.6889 Validation Loss: 0.7982, Accuracy: 0.6458, F1 Score: 0.6306\n",
      "Batch 40 - Avg Train Loss: 0.6122 Validation Loss: 0.8032, Accuracy: 0.6513, F1 Score: 0.6377\n",
      "Batch 50 - Avg Train Loss: 0.5848 Validation Loss: 0.8033, Accuracy: 0.6605, F1 Score: 0.6508\n",
      "Batch 60 - Avg Train Loss: 0.6080 Validation Loss: 0.8342, Accuracy: 0.6402, F1 Score: 0.6396\n",
      "Batch 70 - Avg Train Loss: 0.6240 Validation Loss: 0.8121, Accuracy: 0.6494, F1 Score: 0.6250\n",
      "Batch 80 - Avg Train Loss: 0.6568 Validation Loss: 0.7962, Accuracy: 0.6753, F1 Score: 0.6723\n",
      "Batch 90 - Avg Train Loss: 0.6304 Validation Loss: 0.7873, Accuracy: 0.6624, F1 Score: 0.6468\n",
      "Batch 100 - Avg Train Loss: 0.7003 Validation Loss: 0.7988, Accuracy: 0.6513, F1 Score: 0.6366\n",
      "Batch 110 - Avg Train Loss: 0.6100 Validation Loss: 0.7875, Accuracy: 0.6642, F1 Score: 0.6536\n",
      "Batch 120 - Avg Train Loss: 0.7004 Validation Loss: 0.7883, Accuracy: 0.6513, F1 Score: 0.6468\n",
      "Batch 130 - Avg Train Loss: 0.6071 Validation Loss: 0.7845, Accuracy: 0.6734, F1 Score: 0.6717\n",
      "Batch 140 - Avg Train Loss: 0.5984 Validation Loss: 0.7779, Accuracy: 0.6605, F1 Score: 0.6439\n",
      "Batch 150 - Avg Train Loss: 0.6097 Validation Loss: 0.7754, Accuracy: 0.6753, F1 Score: 0.6696\n",
      "Batch 160 - Avg Train Loss: 0.5884 Validation Loss: 0.7899, Accuracy: 0.6402, F1 Score: 0.6219\n",
      "Batch 170 - Avg Train Loss: 0.6472 Validation Loss: 0.7746, Accuracy: 0.6605, F1 Score: 0.6517\n",
      "Batch 180 - Avg Train Loss: 0.6597 Validation Loss: 0.8264, Accuracy: 0.6531, F1 Score: 0.6448\n",
      "Batch 190 - Avg Train Loss: 0.6306 Validation Loss: 0.7938, Accuracy: 0.6550, F1 Score: 0.6327\n",
      "Batch 200 - Avg Train Loss: 0.5527 Validation Loss: 0.7918, Accuracy: 0.6587, F1 Score: 0.6580\n",
      "Batch 210 - Avg Train Loss: 0.5841 Validation Loss: 0.7844, Accuracy: 0.6642, F1 Score: 0.6546\n",
      "Batch 220 - Avg Train Loss: 0.6403 Validation Loss: 0.7944, Accuracy: 0.6624, F1 Score: 0.6455\n",
      "Batch 230 - Avg Train Loss: 0.7398 Validation Loss: 0.8481, Accuracy: 0.6402, F1 Score: 0.6458\n",
      "Batch 240 - Avg Train Loss: 0.6537 Validation Loss: 0.7782, Accuracy: 0.6531, F1 Score: 0.6441\n",
      "Batch 250 - Avg Train Loss: 0.6706 Validation Loss: 0.7773, Accuracy: 0.6679, F1 Score: 0.6568\n",
      "Batch 260 - Avg Train Loss: 0.6913 Validation Loss: 0.7732, Accuracy: 0.6771, F1 Score: 0.6688\n",
      "Batch 270 - Avg Train Loss: 0.6352 Validation Loss: 0.7705, Accuracy: 0.6716, F1 Score: 0.6644\n",
      "Epoch 3 completed, training metrics recorded.\n",
      "Batch 10 - Avg Train Loss: 0.6508 Validation Loss: 0.7922, Accuracy: 0.6661, F1 Score: 0.6648\n",
      "Batch 20 - Avg Train Loss: 0.5440 Validation Loss: 0.7890, Accuracy: 0.6716, F1 Score: 0.6622\n",
      "Batch 30 - Avg Train Loss: 0.4653 Validation Loss: 0.7841, Accuracy: 0.6753, F1 Score: 0.6716\n",
      "Batch 40 - Avg Train Loss: 0.5096 Validation Loss: 0.7801, Accuracy: 0.6808, F1 Score: 0.6790\n",
      "Batch 50 - Avg Train Loss: 0.4725 Validation Loss: 0.7991, Accuracy: 0.6716, F1 Score: 0.6569\n",
      "Batch 60 - Avg Train Loss: 0.4405 Validation Loss: 0.7740, Accuracy: 0.6679, F1 Score: 0.6608\n",
      "Batch 70 - Avg Train Loss: 0.4264 Validation Loss: 0.7924, Accuracy: 0.6753, F1 Score: 0.6584\n",
      "Batch 80 - Avg Train Loss: 0.4787 Validation Loss: 0.7789, Accuracy: 0.6771, F1 Score: 0.6694\n",
      "Batch 90 - Avg Train Loss: 0.5010 Validation Loss: 0.7962, Accuracy: 0.6697, F1 Score: 0.6722\n",
      "Batch 100 - Avg Train Loss: 0.4552 Validation Loss: 0.7780, Accuracy: 0.6734, F1 Score: 0.6635\n",
      "Batch 110 - Avg Train Loss: 0.4314 Validation Loss: 0.7914, Accuracy: 0.6734, F1 Score: 0.6685\n",
      "Batch 120 - Avg Train Loss: 0.4491 Validation Loss: 0.7811, Accuracy: 0.6863, F1 Score: 0.6759\n",
      "Batch 130 - Avg Train Loss: 0.4785 Validation Loss: 0.7863, Accuracy: 0.6753, F1 Score: 0.6567\n",
      "Batch 140 - Avg Train Loss: 0.4597 Validation Loss: 0.8211, Accuracy: 0.6402, F1 Score: 0.6402\n",
      "Batch 150 - Avg Train Loss: 0.5144 Validation Loss: 0.7709, Accuracy: 0.6827, F1 Score: 0.6696\n",
      "Batch 160 - Avg Train Loss: 0.4437 Validation Loss: 0.9117, Accuracy: 0.6513, F1 Score: 0.6413\n",
      "Batch 170 - Avg Train Loss: 0.5548 Validation Loss: 0.7863, Accuracy: 0.6661, F1 Score: 0.6485\n",
      "Batch 180 - Avg Train Loss: 0.4748 Validation Loss: 0.7578, Accuracy: 0.6882, F1 Score: 0.6825\n",
      "Batch 190 - Avg Train Loss: 0.4512 Validation Loss: 0.7851, Accuracy: 0.6827, F1 Score: 0.6850\n",
      "Batch 200 - Avg Train Loss: 0.4235 Validation Loss: 0.7809, Accuracy: 0.6882, F1 Score: 0.6800\n",
      "Batch 210 - Avg Train Loss: 0.4185 Validation Loss: 0.7633, Accuracy: 0.6956, F1 Score: 0.6920\n",
      "Batch 220 - Avg Train Loss: 0.4552 Validation Loss: 0.7574, Accuracy: 0.6956, F1 Score: 0.6924\n",
      "Batch 230 - Avg Train Loss: 0.4211 Validation Loss: 0.7864, Accuracy: 0.6845, F1 Score: 0.6668\n",
      "Batch 240 - Avg Train Loss: 0.4320 Validation Loss: 0.7588, Accuracy: 0.6956, F1 Score: 0.6924\n",
      "Batch 250 - Avg Train Loss: 0.4468 Validation Loss: 0.7789, Accuracy: 0.7011, F1 Score: 0.6915\n",
      "Batch 260 - Avg Train Loss: 0.5195 Validation Loss: 0.7887, Accuracy: 0.6845, F1 Score: 0.6839\n",
      "Batch 270 - Avg Train Loss: 0.3839 Validation Loss: 0.7815, Accuracy: 0.6919, F1 Score: 0.6856\n",
      "Epoch 4 completed, training metrics recorded.\n",
      "Batch 10 - Avg Train Loss: 0.5942 Validation Loss: 0.8109, Accuracy: 0.6882, F1 Score: 0.6748\n",
      "Batch 20 - Avg Train Loss: 0.3188 Validation Loss: 0.7811, Accuracy: 0.6974, F1 Score: 0.6939\n",
      "Batch 30 - Avg Train Loss: 0.3129 Validation Loss: 0.8003, Accuracy: 0.7048, F1 Score: 0.7003\n",
      "Batch 40 - Avg Train Loss: 0.2724 Validation Loss: 0.8102, Accuracy: 0.6845, F1 Score: 0.6823\n",
      "Batch 50 - Avg Train Loss: 0.2268 Validation Loss: 0.8265, Accuracy: 0.6900, F1 Score: 0.6856\n",
      "Batch 60 - Avg Train Loss: 0.2797 Validation Loss: 0.8255, Accuracy: 0.6863, F1 Score: 0.6795\n",
      "Batch 70 - Avg Train Loss: 0.2618 Validation Loss: 0.8374, Accuracy: 0.6900, F1 Score: 0.6844\n",
      "Batch 80 - Avg Train Loss: 0.3132 Validation Loss: 0.8455, Accuracy: 0.6974, F1 Score: 0.6884\n",
      "Batch 90 - Avg Train Loss: 0.2677 Validation Loss: 0.8419, Accuracy: 0.6956, F1 Score: 0.6900\n",
      "Batch 100 - Avg Train Loss: 0.2151 Validation Loss: 0.8446, Accuracy: 0.6993, F1 Score: 0.6981\n",
      "Batch 110 - Avg Train Loss: 0.2322 Validation Loss: 0.8364, Accuracy: 0.6993, F1 Score: 0.6920\n",
      "Batch 120 - Avg Train Loss: 0.2959 Validation Loss: 0.8462, Accuracy: 0.6937, F1 Score: 0.6957\n",
      "Batch 130 - Avg Train Loss: 0.3027 Validation Loss: 0.8452, Accuracy: 0.7011, F1 Score: 0.6887\n",
      "Batch 140 - Avg Train Loss: 0.2777 Validation Loss: 0.8504, Accuracy: 0.6863, F1 Score: 0.6865\n",
      "Batch 150 - Avg Train Loss: 0.2696 Validation Loss: 0.8556, Accuracy: 0.6937, F1 Score: 0.6812\n",
      "Batch 160 - Avg Train Loss: 0.2145 Validation Loss: 0.8659, Accuracy: 0.6993, F1 Score: 0.6958\n",
      "Batch 170 - Avg Train Loss: 0.2865 Validation Loss: 0.8531, Accuracy: 0.7066, F1 Score: 0.6970\n",
      "Batch 180 - Avg Train Loss: 0.2955 Validation Loss: 0.8502, Accuracy: 0.7011, F1 Score: 0.6958\n",
      "Batch 190 - Avg Train Loss: 0.2491 Validation Loss: 0.8428, Accuracy: 0.7177, F1 Score: 0.7095\n",
      "Batch 200 - Avg Train Loss: 0.2371 Validation Loss: 0.8386, Accuracy: 0.7103, F1 Score: 0.7083\n",
      "Batch 210 - Avg Train Loss: 0.3167 Validation Loss: 0.8437, Accuracy: 0.7159, F1 Score: 0.7071\n",
      "Batch 220 - Avg Train Loss: 0.2673 Validation Loss: 0.8355, Accuracy: 0.7085, F1 Score: 0.7021\n",
      "Batch 230 - Avg Train Loss: 0.3179 Validation Loss: 0.8239, Accuracy: 0.6937, F1 Score: 0.6931\n",
      "Batch 240 - Avg Train Loss: 0.2795 Validation Loss: 0.8402, Accuracy: 0.7030, F1 Score: 0.6971\n",
      "Batch 250 - Avg Train Loss: 0.2255 Validation Loss: 0.8291, Accuracy: 0.7048, F1 Score: 0.7031\n",
      "Batch 260 - Avg Train Loss: 0.2179 Validation Loss: 0.8449, Accuracy: 0.7085, F1 Score: 0.7011\n",
      "Batch 270 - Avg Train Loss: 0.2785 Validation Loss: 0.8464, Accuracy: 0.6974, F1 Score: 0.6985\n",
      "Epoch 5 completed, training metrics recorded.\n",
      "Total training time: 612.57 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "training_metrics = train(\n",
    "    model, train_datast, val_datast, epochs=3, batch_size=16, lr=1e-5, log_freq=10\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "total_training_time = end_time - start_time\n",
    "print(f\"Total training time: {total_training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8dd0ab1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:03<00:00, 10.12batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**inference_time** for batch of size 16: 0.0177 seconds\n",
      "Test accuracy: 0.7090\n",
      "Test f1: 0.7092\n"
     ]
    }
   ],
   "source": [
    "training_metrics.to_pd().to_csv(\n",
    "    \"results/finetune_model_training_metrics.csv\", index=False\n",
    ")\n",
    "\n",
    "y_pred_all, avg_batch_time = inference_with_timing(model, test_loader, device=device)\n",
    "\n",
    "print(f\"**inference_time** for batch of size 16: {avg_batch_time:.4f} seconds\")\n",
    "print(\n",
    "    f\"Test accuracy: {accuracy_score(test_dataset.df['label'], y_pred_all.numpy()):.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Test f1: {f1_score(test_dataset.df['label'], y_pred_all.numpy(), average='weighted'):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8105fd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "test_text = \"bambik z ciebie jest leszczu\"\n",
    "\n",
    "tok = tokenizer(\n",
    "    test_text,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "input_ids = tok[\"input_ids\"]\n",
    "attention_mask = tok[\"attention_mask\"]\n",
    "\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "model.to(device)\n",
    "\n",
    "probs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "print(torch.argmax(probs, dim=1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433a99b0",
   "metadata": {},
   "source": [
    "**summary on finetunning:**\n",
    "\n",
    "\n",
    "Training time: 397.82s\n",
    "\n",
    "Average inference_time per batch of size 16: 0.0177 seconds\n",
    "\n",
    "Test accuracy: 0.7090\n",
    "\n",
    "Test f1: 0.7092"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e03f4a",
   "metadata": {},
   "source": [
    "### Additional model\n",
    "\n",
    "Get as best model as you can.\n",
    "\n",
    "Plan: \n",
    "- finetune bert on contrastive learning with word <-> meaning\n",
    "- then train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32a0223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlangContrastiveDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=128):\n",
    "        if isinstance(data, str):\n",
    "            self.df = pd.read_csv(data)\n",
    "        else:\n",
    "            self.df = data\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        word = row[\"word\"]\n",
    "        meaning = row[\"meaning\"]\n",
    "\n",
    "        word_enc = self.tokenizer(\n",
    "            word,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        meaning_enc = self.tokenizer(\n",
    "            meaning,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"word_input_ids\": word_enc[\"input_ids\"].squeeze(),\n",
    "            \"word_attention_mask\": word_enc[\"attention_mask\"].squeeze(),\n",
    "            \"meaning_input_ids\": meaning_enc[\"input_ids\"].squeeze(),\n",
    "            \"meaning_attention_mask\": meaning_enc[\"attention_mask\"].squeeze(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48bd4a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contrastive Learning Dataset size: 4878\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"prepared_data/train.csv\")\n",
    "eval_df = pd.read_csv(\"prepared_data/val.csv\")\n",
    "\n",
    "merged_df = pd.concat([train_df, eval_df], ignore_index=True)\n",
    "\n",
    "contrastive_learning_dataset = SlangContrastiveDataset(merged_df, tokenizer)\n",
    "\n",
    "print(f\"Contrastive Learning Dataset size: {len(contrastive_learning_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9130ba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def contrastive_loss(e_word, e_meaning, temperature=0.07):\n",
    "    sim = torch.matmul(e_word, e_meaning.T)\n",
    "    sim = sim / temperature\n",
    "\n",
    "    labels = torch.arange(\n",
    "        sim.size(0), device=sim.device\n",
    "    ).long()  # on diagonal good pairs\n",
    "    loss = F.cross_entropy(sim, labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab578f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss_training(\n",
    "    model,\n",
    "    dataset,\n",
    "    epochs=3,\n",
    "    batch_size=16,\n",
    "    lr=3e-6,\n",
    "    accumulation_steps=4,\n",
    "    log_freq=10,\n",
    "    temperature=0.07,\n",
    "):\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
    "    metrics = TrainingMetrics()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            word_ids = batch[\"word_input_ids\"].to(device)\n",
    "            word_mask = batch[\"word_attention_mask\"].to(device)\n",
    "            meaning_ids = batch[\"meaning_input_ids\"].to(device)\n",
    "            meaning_mask = batch[\"meaning_attention_mask\"].to(device)\n",
    "\n",
    "            e_word = model(word_ids, word_mask)\n",
    "            e_meaning = model(meaning_ids, meaning_mask)\n",
    "\n",
    "            loss = contrastive_loss(e_word, e_meaning, temperature=temperature)\n",
    "            loss = loss / accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_train_loss += loss.item() * accumulation_steps\n",
    "\n",
    "            if (batch_idx + 1) % log_freq == 0:\n",
    "                avg_train_loss = total_train_loss / log_freq\n",
    "                total_train_loss = 0.0\n",
    "\n",
    "                print(f\"Batch {batch_idx + 1} - Avg Train Loss: {avg_train_loss:.4f} \")\n",
    "                metrics.insert(avg_train_loss, \"\", \"\", \"\")\n",
    "\n",
    "        if (batch_idx + 1) % accumulation_steps != 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} completed.\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ee30655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_contrastive_model(model, tokenizer=None, path=\"models\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{path}/contrastive_finetunned.pt\")\n",
    "\n",
    "    if tokenizer is not None:\n",
    "        tokenizer.save_pretrained(path)\n",
    "\n",
    "    print(f\"Model saved to: {path}\")\n",
    "\n",
    "\n",
    "def load_contrastive_model(model_class, path=\"models\", tokenizer_class=None):\n",
    "    model = model_class()\n",
    "    model.load_state_dict(\n",
    "        torch.load(os.path.join(path, \"contrastive_finetunned.pt\"), map_location=\"cpu\")\n",
    "    )\n",
    "\n",
    "    tokenizer = None\n",
    "    if tokenizer_class is not None:\n",
    "        tokenizer = tokenizer_class.from_pretrained(path)\n",
    "\n",
    "    print(f\"Model loaded from: {path}\")\n",
    "    if tokenizer is not None:\n",
    "        print(\"Tokenizer loaded as well.\")\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5538f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstrastiveFinetuneModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"allegro/herbert-large-cased\",\n",
    "        dropout=0.15,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.model.config.hidden_size\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for layer_idx in range(8, 12):\n",
    "            for param in self.model.encoder.layer[layer_idx].parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        self.projection = nn.Sequential(nn.Linear(hidden_size, 256), nn.LayerNorm(256))\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        last_hidden = self.model(\n",
    "            input_ids=input_ids, attention_mask=attention_mask\n",
    "        ).last_hidden_state\n",
    "        cls_emb = last_hidden[:, 0, :]\n",
    "        projected = self.projection(cls_emb)\n",
    "        normalized = F.normalize(projected, p=2, dim=1)\n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ec010f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allegro/herbert-large-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConstrastiveFinetuneModel(\n",
       "  (model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(50000, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (projection): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrastive_model = ConstrastiveFinetuneModel()\n",
    "contrastive_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84cccf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5 - Avg Train Loss: 4.5703 \n",
      "Batch 10 - Avg Train Loss: 4.5543 \n",
      "Batch 15 - Avg Train Loss: 4.5560 \n",
      "Batch 20 - Avg Train Loss: 4.5730 \n",
      "Batch 25 - Avg Train Loss: 4.5590 \n",
      "Batch 30 - Avg Train Loss: 4.5614 \n",
      "Batch 35 - Avg Train Loss: 4.5617 \n",
      "Batch 40 - Avg Train Loss: 4.5646 \n",
      "Batch 45 - Avg Train Loss: 4.5619 \n",
      "Batch 50 - Avg Train Loss: 4.5601 \n",
      "Epoch 1 completed.\n",
      "Batch 5 - Avg Train Loss: 4.5697 \n",
      "Batch 10 - Avg Train Loss: 4.5604 \n",
      "Batch 15 - Avg Train Loss: 4.5653 \n",
      "Batch 20 - Avg Train Loss: 4.5604 \n",
      "Batch 25 - Avg Train Loss: 4.5570 \n",
      "Batch 30 - Avg Train Loss: 4.5515 \n",
      "Batch 35 - Avg Train Loss: 4.5553 \n",
      "Batch 40 - Avg Train Loss: 4.5548 \n",
      "Batch 45 - Avg Train Loss: 4.5455 \n",
      "Batch 50 - Avg Train Loss: 4.5505 \n",
      "Epoch 2 completed.\n",
      "Batch 5 - Avg Train Loss: 4.5592 \n",
      "Batch 10 - Avg Train Loss: 4.5496 \n",
      "Batch 15 - Avg Train Loss: 4.5474 \n",
      "Batch 20 - Avg Train Loss: 4.5399 \n",
      "Batch 25 - Avg Train Loss: 4.5405 \n",
      "Batch 30 - Avg Train Loss: 4.5551 \n",
      "Batch 35 - Avg Train Loss: 4.5438 \n",
      "Batch 40 - Avg Train Loss: 4.5429 \n",
      "Batch 45 - Avg Train Loss: 4.5391 \n",
      "Batch 50 - Avg Train Loss: 4.5398 \n",
      "Epoch 3 completed.\n",
      "Batch 5 - Avg Train Loss: 4.5322 \n",
      "Batch 10 - Avg Train Loss: 4.5362 \n",
      "Batch 15 - Avg Train Loss: 4.5305 \n",
      "Batch 20 - Avg Train Loss: 4.5385 \n",
      "Batch 25 - Avg Train Loss: 4.5328 \n",
      "Batch 30 - Avg Train Loss: 4.5252 \n",
      "Batch 35 - Avg Train Loss: 4.5402 \n",
      "Batch 40 - Avg Train Loss: 4.5340 \n",
      "Batch 45 - Avg Train Loss: 4.5273 \n",
      "Batch 50 - Avg Train Loss: 4.5160 \n",
      "Epoch 4 completed.\n",
      "Batch 5 - Avg Train Loss: 4.5197 \n",
      "Batch 10 - Avg Train Loss: 4.5154 \n",
      "Batch 15 - Avg Train Loss: 4.5233 \n",
      "Batch 20 - Avg Train Loss: 4.5039 \n",
      "Batch 25 - Avg Train Loss: 4.5027 \n",
      "Batch 30 - Avg Train Loss: 4.5001 \n",
      "Batch 35 - Avg Train Loss: 4.5124 \n",
      "Batch 40 - Avg Train Loss: 4.4999 \n",
      "Batch 45 - Avg Train Loss: 4.5311 \n",
      "Batch 50 - Avg Train Loss: 4.4964 \n",
      "Epoch 5 completed.\n",
      "Batch 5 - Avg Train Loss: 4.4629 \n",
      "Batch 10 - Avg Train Loss: 4.4808 \n",
      "Batch 15 - Avg Train Loss: 4.4377 \n",
      "Batch 20 - Avg Train Loss: 4.4945 \n",
      "Batch 25 - Avg Train Loss: 4.4508 \n",
      "Batch 30 - Avg Train Loss: 4.4727 \n",
      "Batch 35 - Avg Train Loss: 4.4526 \n",
      "Batch 40 - Avg Train Loss: 4.4517 \n",
      "Batch 45 - Avg Train Loss: 4.4669 \n",
      "Batch 50 - Avg Train Loss: 4.4602 \n",
      "Epoch 6 completed.\n"
     ]
    }
   ],
   "source": [
    "contrastive_learning_metrics = contrastive_loss_training(\n",
    "    contrastive_model,\n",
    "    contrastive_learning_dataset,\n",
    "    epochs=6,\n",
    "    batch_size=96,\n",
    "    accumulation_steps=1,\n",
    "    lr=5e-6,\n",
    "    log_freq=5,\n",
    "    temperature=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd6183d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: models\n"
     ]
    }
   ],
   "source": [
    "contrastive_learning_metrics.to_pd().to_csv(\n",
    "    \"results/contrastive_model_training_metrics.csv\", index=False\n",
    ")\n",
    "save_contrastive_model(contrastive_model, tokenizer, path=\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e14923",
   "metadata": {},
   "source": [
    "### Finetunning with contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07fa230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveFinetunedClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path=\"models\",\n",
    "        num_labels=3,\n",
    "        dropout=0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = load_contrastive_model(ConstrastiveFinetuneModel, path=model_path)[\n",
    "            0\n",
    "        ]\n",
    "        hidden_size = 256\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for layer_idx in range(8, 12):\n",
    "            for param in self.model.model.encoder.layer[layer_idx].parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        for param in self.model.projection.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        cls = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x = self.dropout(cls)\n",
    "        logits = self.classifier(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30fce9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allegro/herbert-large-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from: models\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ContrastiveFinetunedClassifier(\n",
       "  (model): ConstrastiveFinetuneModel(\n",
       "    (model): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(50000, 1024, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 1024)\n",
       "        (token_type_embeddings): Embedding(2, 1024)\n",
       "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-23): 24 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (projection): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=256, out_features=3, bias=True)\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constastive_calsifier = ContrastiveFinetunedClassifier()\n",
    "constastive_calsifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73d17b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10 - Avg Train Loss: 1.1160 Validation Loss: 1.1080, Accuracy: 0.2103, F1 Score: 0.0731\n",
      "Batch 20 - Avg Train Loss: 1.1066 Validation Loss: 1.1032, Accuracy: 0.2122, F1 Score: 0.0801\n",
      "Batch 30 - Avg Train Loss: 1.0998 Validation Loss: 1.0991, Accuracy: 0.3044, F1 Score: 0.2783\n",
      "Batch 40 - Avg Train Loss: 1.0970 Validation Loss: 1.0938, Accuracy: 0.4871, F1 Score: 0.4046\n",
      "Batch 50 - Avg Train Loss: 1.0927 Validation Loss: 1.0883, Accuracy: 0.4779, F1 Score: 0.3345\n",
      "Batch 60 - Avg Train Loss: 1.0797 Validation Loss: 1.0784, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 70 - Avg Train Loss: 1.0862 Validation Loss: 1.0718, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 80 - Avg Train Loss: 1.0775 Validation Loss: 1.0660, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 90 - Avg Train Loss: 1.0551 Validation Loss: 1.0580, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 100 - Avg Train Loss: 1.0496 Validation Loss: 1.0494, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 110 - Avg Train Loss: 1.0482 Validation Loss: 1.0426, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 120 - Avg Train Loss: 1.0081 Validation Loss: 1.0379, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 130 - Avg Train Loss: 1.0263 Validation Loss: 1.0348, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 140 - Avg Train Loss: 1.0611 Validation Loss: 1.0321, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 150 - Avg Train Loss: 1.0389 Validation Loss: 1.0275, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 160 - Avg Train Loss: 1.0102 Validation Loss: 1.0210, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 170 - Avg Train Loss: 1.0153 Validation Loss: 1.0175, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 180 - Avg Train Loss: 1.0149 Validation Loss: 1.0286, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 190 - Avg Train Loss: 1.0121 Validation Loss: 1.0254, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 200 - Avg Train Loss: 1.0123 Validation Loss: 1.0227, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 210 - Avg Train Loss: 0.9913 Validation Loss: 1.0205, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 220 - Avg Train Loss: 1.0219 Validation Loss: 1.0183, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 230 - Avg Train Loss: 0.9999 Validation Loss: 1.0084, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 240 - Avg Train Loss: 0.9928 Validation Loss: 1.0099, Accuracy: 0.5037, F1 Score: 0.3374\n",
      "Batch 250 - Avg Train Loss: 1.0370 Validation Loss: 1.0040, Accuracy: 0.5018, F1 Score: 0.3366\n",
      "Batch 260 - Avg Train Loss: 0.9920 Validation Loss: 1.0024, Accuracy: 0.5018, F1 Score: 0.3366\n",
      "Batch 270 - Avg Train Loss: 0.9861 Validation Loss: 0.9946, Accuracy: 0.5074, F1 Score: 0.3515\n",
      "Epoch 1 completed, training metrics recorded.\n",
      "Batch 10 - Avg Train Loss: 0.9967 Validation Loss: 0.9932, Accuracy: 0.5332, F1 Score: 0.4000\n",
      "Batch 20 - Avg Train Loss: 1.0095 Validation Loss: 0.9899, Accuracy: 0.5590, F1 Score: 0.4409\n",
      "Batch 30 - Avg Train Loss: 0.9701 Validation Loss: 0.9835, Accuracy: 0.5701, F1 Score: 0.4605\n",
      "Batch 40 - Avg Train Loss: 0.9579 Validation Loss: 0.9881, Accuracy: 0.5627, F1 Score: 0.4477\n",
      "Batch 50 - Avg Train Loss: 0.9892 Validation Loss: 0.9775, Accuracy: 0.5627, F1 Score: 0.4549\n",
      "Batch 60 - Avg Train Loss: 0.9765 Validation Loss: 0.9735, Accuracy: 0.5701, F1 Score: 0.4614\n",
      "Batch 70 - Avg Train Loss: 0.9730 Validation Loss: 0.9707, Accuracy: 0.5701, F1 Score: 0.4608\n",
      "Batch 80 - Avg Train Loss: 0.9615 Validation Loss: 0.9708, Accuracy: 0.5775, F1 Score: 0.4686\n",
      "Batch 90 - Avg Train Loss: 0.9904 Validation Loss: 0.9616, Accuracy: 0.5867, F1 Score: 0.4876\n",
      "Batch 100 - Avg Train Loss: 0.9477 Validation Loss: 0.9616, Accuracy: 0.5867, F1 Score: 0.4784\n",
      "Batch 110 - Avg Train Loss: 0.9594 Validation Loss: 0.9546, Accuracy: 0.5959, F1 Score: 0.4892\n",
      "Batch 120 - Avg Train Loss: 0.9322 Validation Loss: 0.9565, Accuracy: 0.5849, F1 Score: 0.4865\n",
      "Batch 130 - Avg Train Loss: 0.9359 Validation Loss: 0.9472, Accuracy: 0.5923, F1 Score: 0.4882\n",
      "Batch 140 - Avg Train Loss: 0.9656 Validation Loss: 0.9487, Accuracy: 0.5978, F1 Score: 0.4914\n",
      "Batch 150 - Avg Train Loss: 0.9496 Validation Loss: 0.9665, Accuracy: 0.5664, F1 Score: 0.4515\n",
      "Batch 160 - Avg Train Loss: 0.9534 Validation Loss: 0.9675, Accuracy: 0.5646, F1 Score: 0.4456\n",
      "Batch 170 - Avg Train Loss: 0.9574 Validation Loss: 0.9394, Accuracy: 0.5978, F1 Score: 0.4960\n",
      "Batch 180 - Avg Train Loss: 0.9470 Validation Loss: 0.9705, Accuracy: 0.5554, F1 Score: 0.4655\n",
      "Batch 190 - Avg Train Loss: 0.9482 Validation Loss: 0.9380, Accuracy: 0.6052, F1 Score: 0.5075\n",
      "Batch 200 - Avg Train Loss: 0.9433 Validation Loss: 0.9465, Accuracy: 0.5978, F1 Score: 0.5018\n",
      "Batch 210 - Avg Train Loss: 0.9364 Validation Loss: 0.9452, Accuracy: 0.6107, F1 Score: 0.5183\n",
      "Batch 220 - Avg Train Loss: 0.9512 Validation Loss: 0.9366, Accuracy: 0.6255, F1 Score: 0.5504\n",
      "Batch 230 - Avg Train Loss: 0.9158 Validation Loss: 0.9275, Accuracy: 0.6236, F1 Score: 0.5631\n",
      "Batch 240 - Avg Train Loss: 0.9238 Validation Loss: 0.9249, Accuracy: 0.6292, F1 Score: 0.5815\n",
      "Batch 250 - Avg Train Loss: 0.9024 Validation Loss: 0.9438, Accuracy: 0.6070, F1 Score: 0.5487\n",
      "Batch 260 - Avg Train Loss: 0.9123 Validation Loss: 0.9276, Accuracy: 0.6273, F1 Score: 0.5584\n",
      "Batch 270 - Avg Train Loss: 0.9201 Validation Loss: 0.9243, Accuracy: 0.6236, F1 Score: 0.5554\n",
      "Epoch 2 completed, training metrics recorded.\n",
      "Batch 10 - Avg Train Loss: 0.9143 Validation Loss: 0.9197, Accuracy: 0.6292, F1 Score: 0.5777\n",
      "Batch 20 - Avg Train Loss: 0.8785 Validation Loss: 0.9159, Accuracy: 0.6292, F1 Score: 0.5900\n",
      "Batch 30 - Avg Train Loss: 0.8759 Validation Loss: 0.9175, Accuracy: 0.6347, F1 Score: 0.5746\n",
      "Batch 40 - Avg Train Loss: 0.9047 Validation Loss: 0.9108, Accuracy: 0.6402, F1 Score: 0.6083\n",
      "Batch 50 - Avg Train Loss: 0.9101 Validation Loss: 0.9209, Accuracy: 0.6273, F1 Score: 0.6192\n",
      "Batch 60 - Avg Train Loss: 0.8941 Validation Loss: 0.9255, Accuracy: 0.6273, F1 Score: 0.5589\n",
      "Batch 70 - Avg Train Loss: 0.9031 Validation Loss: 0.9154, Accuracy: 0.6310, F1 Score: 0.5537\n",
      "Batch 80 - Avg Train Loss: 0.8700 Validation Loss: 0.9076, Accuracy: 0.6550, F1 Score: 0.6009\n",
      "Batch 90 - Avg Train Loss: 0.8760 Validation Loss: 0.9055, Accuracy: 0.6513, F1 Score: 0.6454\n",
      "Batch 100 - Avg Train Loss: 0.8730 Validation Loss: 0.9192, Accuracy: 0.6292, F1 Score: 0.6362\n",
      "Batch 110 - Avg Train Loss: 0.8708 Validation Loss: 0.8995, Accuracy: 0.6642, F1 Score: 0.6487\n",
      "Batch 120 - Avg Train Loss: 0.8845 Validation Loss: 0.9035, Accuracy: 0.6421, F1 Score: 0.5959\n",
      "Batch 130 - Avg Train Loss: 0.8713 Validation Loss: 0.9036, Accuracy: 0.6513, F1 Score: 0.6337\n",
      "Batch 140 - Avg Train Loss: 0.8645 Validation Loss: 0.9056, Accuracy: 0.6384, F1 Score: 0.6365\n",
      "Batch 150 - Avg Train Loss: 0.8996 Validation Loss: 0.9033, Accuracy: 0.6531, F1 Score: 0.6355\n",
      "Batch 160 - Avg Train Loss: 0.9077 Validation Loss: 0.9107, Accuracy: 0.6458, F1 Score: 0.6152\n",
      "Batch 170 - Avg Train Loss: 0.8876 Validation Loss: 0.9002, Accuracy: 0.6587, F1 Score: 0.6335\n",
      "Batch 180 - Avg Train Loss: 0.8516 Validation Loss: 0.8988, Accuracy: 0.6476, F1 Score: 0.6412\n",
      "Batch 190 - Avg Train Loss: 0.8925 Validation Loss: 0.8962, Accuracy: 0.6697, F1 Score: 0.6503\n",
      "Batch 200 - Avg Train Loss: 0.8237 Validation Loss: 0.8945, Accuracy: 0.6513, F1 Score: 0.6446\n",
      "Batch 210 - Avg Train Loss: 0.8891 Validation Loss: 0.8931, Accuracy: 0.6513, F1 Score: 0.6452\n",
      "Batch 220 - Avg Train Loss: 0.8965 Validation Loss: 0.8960, Accuracy: 0.6494, F1 Score: 0.6386\n",
      "Batch 230 - Avg Train Loss: 0.8490 Validation Loss: 0.8935, Accuracy: 0.6587, F1 Score: 0.6455\n",
      "Batch 240 - Avg Train Loss: 0.8417 Validation Loss: 0.8948, Accuracy: 0.6476, F1 Score: 0.6463\n",
      "Batch 250 - Avg Train Loss: 0.8538 Validation Loss: 0.8899, Accuracy: 0.6605, F1 Score: 0.6528\n",
      "Batch 260 - Avg Train Loss: 0.8637 Validation Loss: 0.8915, Accuracy: 0.6550, F1 Score: 0.6196\n",
      "Batch 270 - Avg Train Loss: 0.8721 Validation Loss: 0.9132, Accuracy: 0.6162, F1 Score: 0.6033\n",
      "Epoch 3 completed, training metrics recorded.\n",
      "Total training time: 366.93 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "training_metrics = train(\n",
    "    constastive_calsifier,\n",
    "    train_datast,\n",
    "    val_datast,\n",
    "    epochs=3,\n",
    "    batch_size=16,\n",
    "    lr=1e-5,\n",
    "    log_freq=10,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "total_training_time = end_time - start_time\n",
    "print(f\"Total training time: {total_training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78e9604e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:03<00:00, 11.05batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**inference_time** for batch of size 16: 0.0137 seconds\n",
      "Test accuracy: 0.6593\n",
      "Test f1: 0.6556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training_metrics.to_pd().to_csv(\n",
    "    \"results/finetune_model_contrastive_training_metrics.csv\", index=False\n",
    ")\n",
    "\n",
    "y_pred_all, avg_batch_time = inference_with_timing(\n",
    "    constastive_calsifier, test_loader, device=device\n",
    ")\n",
    "\n",
    "print(f\"**inference_time** for batch of size 16: {avg_batch_time:.4f} seconds\")\n",
    "print(\n",
    "    f\"Test accuracy: {accuracy_score(test_dataset.df['label'], y_pred_all.numpy()):.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Test f1: {f1_score(test_dataset.df['label'], y_pred_all.numpy(), average='weighted'):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49adc421",
   "metadata": {},
   "source": [
    "**summary on contrastive finetunned**\n",
    "\n",
    "Training time: 366.93s\n",
    "\n",
    "Average inference_time per batch of size 16: 0.0137 seconds\n",
    "\n",
    "Test accuracy: 0.6593\n",
    "\n",
    "Test f1: 0.6556"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computional-linguistic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
